{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from squlearn.qnn.loss import *\n",
    "from squlearn.qnn.training import *\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom squlearn import Executor\\nfrom squlearn.encoding_circuit import ChebyshevRx, HubregtsenEncodingCircuit,YZ_CX_EncodingCircuit\\nfrom squlearn.observables import SummedPaulis\\nfrom squlearn.qnn import QNNClassifier, SquaredLoss, QNNRegressor, ODELoss\\nfrom squlearn.optimizers import *\\nfrom sklearn.datasets import make_blobs\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import MinMaxScaler\\nimport numpy as np\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from squlearn import Executor\n",
    "from squlearn.encoding_circuit import ChebyshevRx, HubregtsenEncodingCircuit,YZ_CX_EncodingCircuit\n",
    "from squlearn.observables import SummedPaulis\n",
    "from squlearn.qnn import QNNClassifier, SquaredLoss, QNNRegressor, ODELoss\n",
    "from squlearn.optimizers import *\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from squlearn import Executor\n",
    "from squlearn.encoding_circuit import ChebyshevRx, YZ_CX_EncodingCircuit\n",
    "from squlearn.observables import SummedPaulis\n",
    "from squlearn.qnn import QNNClassifier, QNNRegressor, SquaredLoss, ODELoss\n",
    "from squlearn.optimizers import SLSQP\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_functional(loss_values):\n",
    "    x = loss_values[\"x\"]\n",
    "    f = loss_values[\"f\"]\n",
    "    dfdx = loss_values[\"dfdx\"][:,0]\n",
    "    dfdxdx = loss_values[\"dfdxdx\"][:,0,0]\n",
    "    value =  dfdx - np.cos(x)\n",
    "    #insert initial value at last position of value\n",
    "    #value = np.append(value, initial_value)\n",
    "    return value\n",
    "\n",
    "def grad_F_functional(loss_values):\n",
    "    \"\"\"ArithmeticError\n",
    "    n = x_span.shape[0] number of points\n",
    "    m = x_span.shape[1] number of dimensions (typically m=1)\n",
    "\n",
    "    F[x, x_, x__] = envelope(x, x_, x__)\n",
    "\n",
    "    grad_envelope = (envelope(x, x_, x__)dx, envelope(x, x_, x__)dx_, envelope(x, x_, x__)dx__)\n",
    "\n",
    "    \"\"\"\n",
    "    x_span = loss_values[\"x\"] # shape (n, m)   \n",
    "    f = loss_values[\"f\"] # shape (n,)\n",
    "    dfdx = loss_values[\"dfdx\"][:,0] # shape (n, m) \n",
    "    dfdxdx = loss_values[\"dfdxdx\"][:,0,:] # shape (n, 1, m)\n",
    "\n",
    "    dfdp = loss_values[\"dfdp\"] # shape (n, p)\n",
    "    n_param = dfdp.shape[1]\n",
    "    #dfdpdx = loss_values[\"dfdpdx\"][:,0,:] # shape (n, 1, p)\n",
    "    #dfdpdxdx = loss_values[\"dfdpdxdx\"][:,0,:] # shape (n, 1, 1, p)\n",
    "    \n",
    "    try:\n",
    "        dfdxdp = loss_values[\"dfdxdp\"] # shape (n, 1, P)\n",
    "        dfdxdxdp = loss_values[\"dfdxdxdp\"] # shape (n, 1, 1, P)\n",
    "    except:\n",
    "        dfdpdx = loss_values[\"dfdpdx\"][:, :, 0] # shape (n, p, 1)\n",
    "        dfdpdxdx = loss_values[\"dfdpdxdx\"][:,:, 0, 0] # shape (n, p, 1, 1)\n",
    "\n",
    "    #problem dependent gradient!!\n",
    "    grad_envelope_list = np.zeros((3, x_span.shape[0], n_param)) # shape (3, n, p) \n",
    "    grad_envelope_list[0,:,:] = 0\n",
    "    grad_envelope_list[1,:,:] = 1\n",
    "    grad_envelope_list[2,:,:] = 0\n",
    "    ###########3\n",
    "\n",
    "\n",
    "\n",
    "    return grad_envelope_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_values = np.linspace(0, 4, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.44444444, 0.88888889, 1.33333333, 1.77777778,\n",
       "       2.22222222, 2.66666667, 3.11111111, 3.55555556, 4.        ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'clf_ODE = QNNRegressor(\\n    encoding_circuit = YZ_CX_EncodingCircuit(num_qubits, num_features, num_layers),\\n    operator = SummedPaulis(num_qubits),\\n    executor = Executor(),\\n    loss = ODELoss(L_functional, grad_F_functional, initial_vec = initial_value, eta=1),\\n    optimizer = SLSQP(),\\n    param_ini = np.random.rand(2*num_qubits*num_layers),\\n    param_op_ini = np.ones(num_qubits+1),\\n    opt_param_op = False\\n)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_qubits = 1\n",
    "num_features = 1\n",
    "num_layers = 4\n",
    "\n",
    "initial_value = np.array([0])\n",
    "\n",
    "\"\"\"clf_ODE = QNNRegressor(\n",
    "    encoding_circuit = YZ_CX_EncodingCircuit(num_qubits, num_features, num_layers),\n",
    "    operator = SummedPaulis(num_qubits),\n",
    "    executor = Executor(),\n",
    "    loss = ODELoss(L_functional, grad_F_functional, initial_vec = initial_value, eta=1),\n",
    "    optimizer = SLSQP(),\n",
    "    param_ini = np.random.rand(2*num_qubits*num_layers),\n",
    "    param_op_ini = np.ones(num_qubits+1),\n",
    "    opt_param_op = False\n",
    ")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = ODELoss(L_functional, grad_F_functional, initial_vec = initial_value, eta=1)\n",
    "loss = SquaredLoss()\n",
    "clf = QNNRegressor(\n",
    "    YZ_CX_EncodingCircuit(num_qubits, num_features, num_layers),\n",
    "    SummedPaulis(num_qubits),\n",
    "    Executor(\"pennylane\"),\n",
    "    loss,\n",
    "    SLSQP(),\n",
    "    np.random.rand(2*num_qubits*num_layers),\n",
    "    np.ones(num_qubits+1),\n",
    "    opt_param_op = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit:   0%|          | 0/100 [00:00<?, ?it/s]Functional loss:  27.477744353854533\n",
      "Initial value loss f:  0.755868482676726\n",
      "Total:  28.233612836531258\n",
      "dfdxdxdp shape:  (10, 1, 1, 8)\n",
      "grad_envelope_list [[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
      "Gradient sum:  -4.129544926617104\n",
      "Functional loss:  31.402459256649657\n",
      "Initial value loss f:  2.818576112127009\n",
      "Total:  34.22103536877667\n",
      "fit:   1%|          | 1/100 [00:09<15:43,  9.53s/it]Functional loss:  16.145619730460663\n",
      "Initial value loss f:  1.4181561267460472\n",
      "Total:  17.56377585720671\n",
      "Functional loss:  17.614697208516233\n",
      "Initial value loss f:  0.4972629055264107\n",
      "Total:  18.111960114042645\n",
      "Functional loss:  26.552279898075703\n",
      "Initial value loss f:  0.12340679944962407\n",
      "Total:  26.67568669752533\n",
      "Functional loss:  17.544858853252727\n",
      "Initial value loss f:  0.010486926468615907\n",
      "Total:  17.555345779721343\n",
      "dfdxdxdp shape:  (10, 1, 1, 8)\n",
      "grad_envelope_list [[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
      "Gradient sum:  -55.05465426808017\n",
      "Functional loss:  39.714151699566386\n",
      "Initial value loss f:  2.355341701201746\n",
      "Total:  42.06949340076813\n",
      "fit:   2%|▏         | 2/100 [00:19<15:46,  9.66s/it]Functional loss:  10.9508576964154\n",
      "Initial value loss f:  3.2981661525619432\n",
      "Total:  14.249023848977343\n",
      "Functional loss:  43.2462531223961\n",
      "Initial value loss f:  0.8532376886791256\n",
      "Total:  44.09949081107522\n",
      "Functional loss:  28.538875420591612\n",
      "Initial value loss f:  0.6837038730012461\n",
      "Total:  29.222579293592858\n",
      "Functional loss:  15.473043905300289\n",
      "Initial value loss f:  0.003203149422681471\n",
      "Total:  15.47624705472297\n",
      "dfdxdxdp shape:  (10, 1, 1, 8)\n",
      "grad_envelope_list [[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
      "Gradient sum:  2.1023542310251795\n",
      "Functional loss:  14.2507674580588\n",
      "Initial value loss f:  1.4821961275730315\n",
      "Total:  15.73296358563183\n",
      "fit:   3%|▎         | 3/100 [00:28<15:28,  9.58s/it]Functional loss:  15.330436559837182\n",
      "Initial value loss f:  1.878941246206922\n",
      "Total:  17.209377806044102\n",
      "Functional loss:  13.999408081463955\n",
      "Initial value loss f:  0.31164074305183326\n",
      "Total:  14.311048824515789\n",
      "dfdxdxdp shape:  (10, 1, 1, 8)\n",
      "grad_envelope_list [[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
      "Gradient sum:  13.710699679026803\n",
      "Functional loss:  22.853728810313726\n",
      "Initial value loss f:  0.025164314697411205\n",
      "Total:  22.878893125011135\n",
      "fit:   4%|▍         | 4/100 [00:38<15:26,  9.65s/it]Functional loss:  15.339915755834774\n",
      "Initial value loss f:  1.4780235390217655\n",
      "Total:  16.81793929485654\n",
      "Functional loss:  11.022275136479365\n",
      "Initial value loss f:  1.2203990830425075\n",
      "Total:  12.242674219521874\n",
      "dfdxdxdp shape:  (10, 1, 1, 8)\n",
      "grad_envelope_list [[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
      "Gradient sum:  29.406263344405417\n",
      "Functional loss:  45.06348501287773\n",
      "Initial value loss f:  2.7246648256574963\n",
      "Total:  47.788149838535226\n",
      "fit:   5%|▌         | 5/100 [00:47<15:03,  9.51s/it]Functional loss:  30.23761848564799\n",
      "Initial value loss f:  1.3023077324877694\n",
      "Total:  31.539926218135758\n",
      "Functional loss:  26.057707686291174\n",
      "Initial value loss f:  0.1831699852724074\n",
      "Total:  26.24087767156358\n",
      "Functional loss:  9.86362331808669\n",
      "Initial value loss f:  0.31862096184110017\n",
      "Total:  10.18224427992779\n",
      "dfdxdxdp shape:  (10, 1, 1, 8)\n",
      "grad_envelope_list [[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
      "Gradient sum:  7.022822547780587\n",
      "Functional loss:  12.333214149348706\n",
      "Initial value loss f:  1.3417110535443362\n",
      "Total:  13.674925202893043\n",
      "fit:   6%|▌         | 6/100 [00:57<14:58,  9.56s/it]Functional loss:  21.116331488470546\n",
      "Initial value loss f:  0.5154040339900099\n",
      "Total:  21.631735522460556\n",
      "Functional loss:  8.110260701220835\n",
      "Initial value loss f:  0.11976463212849224\n",
      "Total:  8.230025333349326\n",
      "dfdxdxdp shape:  (10, 1, 1, 8)\n",
      "grad_envelope_list [[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
      "Gradient sum:  7.203826288202929\n",
      "Functional loss:  13.79062906970712\n",
      "Initial value loss f:  0.03279034978091713\n",
      "Total:  13.823419419488037\n",
      "fit:   7%|▋         | 7/100 [01:06<14:37,  9.43s/it]Functional loss:  4.890140246400603\n",
      "Initial value loss f:  0.19760045318417768\n",
      "Total:  5.0877406995847805\n",
      "dfdxdxdp shape:  (10, 1, 1, 8)\n",
      "grad_envelope_list [[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
      "Gradient sum:  1.9717735849904567\n",
      "Functional loss:  15.625905488691176\n",
      "Initial value loss f:  2.994017258229198\n",
      "Total:  18.619922746920373\n",
      "fit:   8%|▊         | 8/100 [01:16<14:42,  9.59s/it]Functional loss:  0.6644427097743786\n",
      "Initial value loss f:  0.5897814685673475\n",
      "Total:  1.254224178341726\n",
      "dfdxdxdp shape:  (10, 1, 1, 8)\n",
      "grad_envelope_list [[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
      "Gradient sum:  12.286591021350546\n",
      "Functional loss:  1.2207665086133637\n",
      "Initial value loss f:  1.901423893934042\n",
      "Total:  3.122190402547406\n",
      "fit:   9%|▉         | 9/100 [01:25<14:12,  9.37s/it]Functional loss:  0.5793429492348042\n",
      "Initial value loss f:  0.8535933805244381\n",
      "Total:  1.4329363297592423\n",
      "Functional loss:  0.6135776985104837\n",
      "Initial value loss f:  0.6856859578001008\n",
      "Total:  1.2992636563105844\n",
      "Functional loss:  0.6388593892107277\n",
      "Initial value loss f:  0.6305555034508159\n",
      "Total:  1.2694148926615436\n",
      "Functional loss:  0.6519646362868519\n",
      "Initial value loss f:  0.6082762943188916\n",
      "Total:  1.2602409306057436\n",
      "Functional loss:  0.6584160119007917\n",
      "Initial value loss f:  0.598420653689067\n",
      "Total:  1.2568366655898586\n",
      "Functional loss:  0.6615433063136792\n",
      "Initial value loss f:  0.593872830156406\n",
      "Total:  1.2554161364700853\n",
      "Functional loss:  0.6630502355662257\n",
      "Initial value loss f:  0.5917317442303233\n",
      "Total:  1.254781979796549\n",
      "Functional loss:  0.663774489564005\n",
      "Initial value loss f:  0.5907140264346069\n",
      "Total:  1.2544885159986119\n",
      "Functional loss:  0.6641221646397407\n",
      "Initial value loss f:  0.5902280515624042\n",
      "Total:  1.254350216202145\n",
      "Functional loss:  0.6642889717241879\n",
      "Initial value loss f:  0.5899954806466319\n",
      "Total:  1.2542844523708196\n",
      "dfdxdxdp shape:  (10, 1, 1, 8)\n",
      "grad_envelope_list [[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
      "Gradient sum:  12.2910004984545\n",
      "Functional loss:  18.423480043423744\n",
      "Initial value loss f:  0.022639701157363815\n",
      "Total:  18.446119744581107\n",
      "fit:  10%|█         | 10/100 [01:39<16:20, 10.89s/it]Functional loss:  6.002919396845462\n",
      "Initial value loss f:  3.819430037351341\n",
      "Total:  9.822349434196802\n",
      "Functional loss:  0.6318238287648996\n",
      "Initial value loss f:  1.060953644512447\n",
      "Total:  1.6927774732773466\n",
      "Functional loss:  0.6246558326042008\n",
      "Initial value loss f:  0.7407647515315042\n",
      "Total:  1.365420584135705\n",
      "Functional loss:  0.6440391380876785\n",
      "Initial value loss f:  0.6498553158734327\n",
      "Total:  1.2938944539611112\n",
      "Functional loss:  0.6546473957987947\n",
      "Initial value loss f:  0.6158376740053183\n",
      "Total:  1.270485069804113\n",
      "Functional loss:  0.6597823573714311\n",
      "Initial value loss f:  0.6015684803042317\n",
      "Total:  1.2613508376756628\n",
      "Functional loss:  0.6621969160956397\n",
      "Initial value loss f:  0.5952649460849306\n",
      "Total:  1.2574618621805702\n",
      "Functional loss:  0.6633206109446298\n",
      "Initial value loss f:  0.592413085134977\n",
      "Total:  1.2557336960796068\n",
      "Functional loss:  0.6638413185565782\n",
      "Initial value loss f:  0.5911085471457521\n",
      "Total:  1.2549498657023304\n",
      "Functional loss:  0.6640821519115973\n",
      "Initial value loss f:  0.5905087630225375\n",
      "Total:  1.2545909149341348\n",
      "dfdxdxdp shape:  (10, 1, 1, 8)\n",
      "grad_envelope_list [[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
      "Gradient sum:  12.300332400552936\n",
      "Functional loss:  32.37335242647301\n",
      "Initial value loss f:  1.2480083260064383\n",
      "Total:  33.62136075247945\n",
      "fit:  11%|█         | 11/100 [01:53<17:31, 11.81s/it]Functional loss:  6.798997264028599\n",
      "Initial value loss f:  2.3646246425494657\n",
      "Total:  9.163621906578065\n",
      "Functional loss:  18.247444484427255\n",
      "Initial value loss f:  0.2992772278045269\n",
      "Total:  18.546721712231783\n",
      "Functional loss:  11.15151682546888\n",
      "Initial value loss f:  3.750427573564536\n",
      "Total:  14.901944399033416\n",
      "Functional loss:  0.6268644441033747\n",
      "Initial value loss f:  1.1064215776645518\n",
      "Total:  1.7332860217679267\n",
      "Functional loss:  0.6169625119420036\n",
      "Initial value loss f:  0.745794154044934\n",
      "Total:  1.3627566659869377\n",
      "Functional loss:  0.6409043940051125\n",
      "Initial value loss f:  0.6510788286155003\n",
      "Total:  1.2919832226206127\n",
      "Functional loss:  0.6531602273428431\n",
      "Initial value loss f:  0.6165969706158821\n",
      "Total:  1.2697571979587252\n",
      "Functional loss:  0.6589646168634352\n",
      "Initial value loss f:  0.6022538899849266\n",
      "Total:  1.261218506848362\n",
      "Functional loss:  0.6616870629530616\n",
      "Initial value loss f:  0.595905632486282\n",
      "Total:  1.2575926954393437\n",
      "Functional loss:  0.6629615755413804\n",
      "Initial value loss f:  0.5930123073300455\n",
      "Total:  1.2559738828714258\n",
      "dfdxdxdp shape:  (10, 1, 1, 8)\n",
      "grad_envelope_list [[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
      "Gradient sum:  12.345660292310688\n",
      "Functional loss:  22.680486077744074\n",
      "Initial value loss f:  3.460338561309731\n",
      "Total:  26.140824639053804\n",
      "fit:  12%|█▏        | 12/100 [02:06<17:38, 12.02s/it]Functional loss:  30.57123003782707\n",
      "Initial value loss f:  0.30366088345054826\n",
      "Total:  30.87489092127762\n",
      "Functional loss:  25.82609688145182\n",
      "Initial value loss f:  2.335238740297184\n",
      "Total:  28.161335621749004\n",
      "Functional loss:  6.75933829271531\n",
      "Initial value loss f:  0.13191203302073412\n",
      "Total:  6.891250325736045\n",
      "Functional loss:  34.03436955647918\n",
      "Initial value loss f:  1.1851577867827994\n",
      "Total:  35.21952734326198\n",
      "Functional loss:  12.995732426228532\n",
      "Initial value loss f:  3.3363632183708414\n",
      "Total:  16.332095644599374\n",
      "Functional loss:  0.6462319858363559\n",
      "Initial value loss f:  1.1771230621131534\n",
      "Total:  1.8233550479495093\n",
      "Functional loss:  0.6142602146613501\n",
      "Initial value loss f:  0.7615722534539056\n",
      "Total:  1.3758324681152556\n",
      "Functional loss:  0.6388610016293497\n",
      "Initial value loss f:  0.6578489037379024\n",
      "Total:  1.2967099053672522\n",
      "Functional loss:  0.6516278568849192\n",
      "Initial value loss f:  0.6207546807577831\n",
      "Total:  1.2723825376427023\n",
      "Functional loss:  0.6576612279894716\n",
      "Initial value loss f:  0.6054577508388017\n",
      "Total:  1.2631189788282733\n",
      "dfdxdxdp shape:  (10, 1, 1, 8)\n",
      "grad_envelope_list [[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
      "Gradient sum:  12.562977026973268\n",
      "Functional loss:  16.08897501530074\n",
      "Initial value loss f:  2.9327638916254313\n",
      "Total:  19.02173890692617\n",
      "fit:  13%|█▎        | 13/100 [02:18<17:43, 12.22s/it]Functional loss:  30.133472995330774\n",
      "Initial value loss f:  0.1312395649037565\n",
      "Total:  30.26471256023453\n",
      "Functional loss:  30.608765352536285\n",
      "Initial value loss f:  1.7127381110511604\n",
      "Total:  32.32150346358745\n",
      "Functional loss:  17.413756944768945\n",
      "Initial value loss f:  0.5425133234030174\n",
      "Total:  17.956270268171963\n",
      "Functional loss:  10.853005209279829\n",
      "Initial value loss f:  2.1081915107833753\n",
      "Total:  12.961196720063203\n",
      "Functional loss:  29.21261752900059\n",
      "Initial value loss f:  3.2855954314995737\n",
      "Total:  32.49821296050016\n",
      "Functional loss:  36.948280362813975\n",
      "Initial value loss f:  1.7201071852718182\n",
      "Total:  38.668387548085796\n",
      "Functional loss:  17.38265069235244\n",
      "Initial value loss f:  1.470133874167066\n",
      "Total:  18.852784566519503\n",
      "Functional loss:  0.822128926660949\n",
      "Initial value loss f:  1.5272111245534088\n",
      "Total:  2.349340051214358\n",
      "Functional loss:  0.6056093395223363\n",
      "Initial value loss f:  0.8313502208021832\n",
      "Total:  1.4369595603245195\n",
      "Functional loss:  0.6300517747086467\n",
      "Initial value loss f:  0.6885093185752249\n",
      "Total:  1.3185610932838716\n",
      "dfdxdxdp shape:  (10, 1, 1, 8)\n",
      "grad_envelope_list [[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
      "Gradient sum:  13.770286134716788\n",
      "Functional loss:  13.53079461085738\n",
      "Initial value loss f:  0.0020176714194063304\n",
      "Total:  13.532812282276785\n",
      "fit:  14%|█▍        | 14/100 [02:31<17:34, 12.27s/it]Functional loss:  25.278416514301398\n",
      "Initial value loss f:  3.9522952365726898\n",
      "Total:  29.230711750874086\n",
      "Functional loss:  9.77279220611297\n",
      "Initial value loss f:  0.11728203938461249\n",
      "Total:  9.890074245497582\n",
      "Functional loss:  16.666602206872387\n",
      "Initial value loss f:  0.641196125362265\n",
      "Total:  17.30779833223465\n",
      "Functional loss:  15.864291584716277\n",
      "Initial value loss f:  2.283730185032209\n",
      "Total:  18.148021769748485\n",
      "Functional loss:  23.35829460303922\n",
      "Initial value loss f:  0.35023666795422104\n",
      "Total:  23.70853127099344\n",
      "Functional loss:  25.020182382048205\n",
      "Initial value loss f:  2.558479526694261\n",
      "Total:  27.578661908742465\n",
      "Functional loss:  39.80578975617384\n",
      "Initial value loss f:  1.2481490867512326\n",
      "Total:  41.05393884292507\n",
      "Functional loss:  4.797021179675949\n",
      "Initial value loss f:  2.3791307932317687\n",
      "Total:  7.176151972907718\n",
      "Functional loss:  18.459662102183433\n",
      "Initial value loss f:  0.3403708061298451\n",
      "Total:  18.800032908313277\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\genericpath.py:30\u001b[0m, in \u001b[0;36misfile\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     st \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] Das System kann die angegebene Datei nicht finden: 'C:\\\\Users\\\\jsl-rf\\\\.qiskit\\\\settings.conf'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((input_values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m----> 2\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\Qiskit\\squlearnRF\\src\\squlearn\\qnn\\qnnr.py:240\u001b[0m, in \u001b[0;36mQNNRegressor._fit\u001b[1;34m(self, X, y, weights)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpbar\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pbar \u001b[38;5;241m=\u001b[39m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_total_iterations, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstdout)\n\u001b[1;32m--> 240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpbar\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pbar\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\Desktop\\Qiskit\\squlearnRF\\src\\squlearn\\qnn\\qnnr.py:222\u001b[0m, in \u001b[0;36mQNNRegressor.partial_fit\u001b[1;34m(self, X, y, weights)\u001b[0m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param_op \u001b[38;5;241m=\u001b[39m train(\n\u001b[0;32m    210\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qnn,\n\u001b[0;32m    211\u001b[0m             X,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    219\u001b[0m             \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    220\u001b[0m         )\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 222\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_qnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m            \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_param\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_param_op\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshot_control\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m            \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_fitted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\Qiskit\\squlearnRF\\src\\squlearn\\qnn\\training.py:367\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(qnn, input_values, ground_truth, param_ini, param_op_ini, loss, optimizer, shot_control, weights, opt_param_op)\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    365\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([])\n\u001b[1;32m--> 367\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_ini\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    370\u001b[0m     result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mx\n",
      "File \u001b[1;32m~\\Desktop\\Qiskit\\squlearnRF\\src\\squlearn\\optimizers\\optimizers_wrapper.py:63\u001b[0m, in \u001b[0;36mSLSQP.minimize\u001b[1;34m(self, fun, x0, grad, bounds)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mminimize\u001b[39m(\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28mself\u001b[39m, fun: \u001b[38;5;28mcallable\u001b[39m, x0: np\u001b[38;5;241m.\u001b[39mndarray, grad: \u001b[38;5;28mcallable\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, bounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     49\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OptimizerResult:\n\u001b[0;32m     50\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03m    Function to minimize a given function using the SLSQP optimizer. Is wrapped from scipy.\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m        Result of the optimization in class:`OptimizerResult` format.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m     scipy_result \u001b[38;5;241m=\u001b[39m \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSLSQP\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m     result \u001b[38;5;241m=\u001b[39m OptimizerResult()\n\u001b[0;32m     74\u001b[0m     result\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m scipy_result\u001b[38;5;241m.\u001b[39mx\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\scipy\\optimize\\_minimize.py:705\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    702\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_cobyla(fun, x0, args, constraints, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    703\u001b[0m                             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mslsqp\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 705\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_slsqp(fun, x0, args, jac, bounds,\n\u001b[0;32m    706\u001b[0m                           constraints, callback\u001b[38;5;241m=\u001b[39mcallback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    707\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrust-constr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    708\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_trustregion_constr(fun, x0, args, jac, hess, hessp,\n\u001b[0;32m    709\u001b[0m                                        bounds, constraints,\n\u001b[0;32m    710\u001b[0m                                        callback\u001b[38;5;241m=\u001b[39mcallback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\scipy\\optimize\\_slsqp_py.py:428\u001b[0m, in \u001b[0;36m_minimize_slsqp\u001b[1;34m(func, x0, args, jac, bounds, constraints, maxiter, ftol, iprint, disp, eps, callback, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    422\u001b[0m slsqp(m, meq, x, xl, xu, fx, c, g, a, acc, majiter, mode, w, jw,\n\u001b[0;32m    423\u001b[0m       alpha, f0, gs, h1, h2, h3, h4, t, t0, tol,\n\u001b[0;32m    424\u001b[0m       iexact, incons, ireset, itermx, line,\n\u001b[0;32m    425\u001b[0m       n1, n2, n3)\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# objective and constraint evaluation required\u001b[39;00m\n\u001b[1;32m--> 428\u001b[0m     fx \u001b[38;5;241m=\u001b[39m \u001b[43mwrapped_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    429\u001b[0m     c \u001b[38;5;241m=\u001b[39m _eval_constraint(x, cons)\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# gradient evaluation required\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\scipy\\optimize\\_optimize.py:346\u001b[0m, in \u001b[0;36m_clip_x_for_func.<locals>.eval\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval\u001b[39m(x):\n\u001b[0;32m    345\u001b[0m     x \u001b[38;5;241m=\u001b[39m _check_clip_x(x, bounds)\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\scipy\\optimize\\_differentiable_functions.py:267\u001b[0m, in \u001b[0;36mScalarFunction.fun\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx):\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x_impl(x)\n\u001b[1;32m--> 267\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\scipy\\optimize\\_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[1;32m--> 251\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\scipy\\optimize\\_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\scipy\\optimize\\_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[1;32m~\\Desktop\\Qiskit\\squlearnRF\\src\\squlearn\\qnn\\training.py:300\u001b[0m, in \u001b[0;36mtrain.<locals>._fun\u001b[1;34m(theta)\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(shot_control, ShotsFromRSTD):\n\u001b[0;32m    298\u001b[0m         shot_control\u001b[38;5;241m.\u001b[39mset_shots_for_loss()\n\u001b[1;32m--> 300\u001b[0m loss_values \u001b[38;5;241m=\u001b[39m \u001b[43mqnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_op_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_args_tuple\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m loss_value \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mvalue(\n\u001b[0;32m    303\u001b[0m     loss_values,\n\u001b[0;32m    304\u001b[0m     ground_truth\u001b[38;5;241m=\u001b[39mground_truth,\n\u001b[0;32m    305\u001b[0m     weights\u001b[38;5;241m=\u001b[39mweights_values,\n\u001b[0;32m    306\u001b[0m     iteration\u001b[38;5;241m=\u001b[39miteration,\n\u001b[0;32m    307\u001b[0m )\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss_value\n",
      "File \u001b[1;32m~\\Desktop\\Qiskit\\squlearnRF\\src\\squlearn\\qnn\\lowlevel_qnn_pennylane.py:389\u001b[0m, in \u001b[0;36mLowLevelQNNPennyLane.evaluate\u001b[1;34m(self, x, param, param_obs, *values)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    382\u001b[0m \n\u001b[0;32m    383\u001b[0m     \u001b[38;5;66;03m# Direct evaluation of the QNN\u001b[39;00m\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (todo_class\u001b[38;5;241m.\u001b[39mreturn_grad_x \u001b[38;5;129;01mand\u001b[39;00m todo_class\u001b[38;5;241m.\u001b[39morder \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m    385\u001b[0m         todo_class\u001b[38;5;241m.\u001b[39morder \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor\u001b[38;5;241m.\u001b[39mshots \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    386\u001b[0m     ):  \u001b[38;5;66;03m# TODO: Should be removed if PennyLane bug 4462 is fixed\u001b[39;00m\n\u001b[0;32m    387\u001b[0m         \u001b[38;5;66;03m# evaluate every single x, param, param_op combination separately\u001b[39;00m\n\u001b[0;32m    388\u001b[0m         \u001b[38;5;66;03m# faster evaluation for higher-order derivatives w.r.t. x\u001b[39;00m\n\u001b[1;32m--> 389\u001b[0m         output \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    390\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate_todo_single_x(\n\u001b[0;32m    391\u001b[0m                 todo_class, x_inp_, param_inp_, param_obs_inp_\n\u001b[0;32m    392\u001b[0m             )\n\u001b[0;32m    393\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m x_inp_ \u001b[38;5;129;01min\u001b[39;00m x_inp\n\u001b[0;32m    394\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m param_inp_ \u001b[38;5;129;01min\u001b[39;00m param_inp\n\u001b[0;32m    395\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m param_obs_inp_ \u001b[38;5;129;01min\u001b[39;00m param_obs_inp\n\u001b[0;32m    396\u001b[0m         ]\n\u001b[0;32m    397\u001b[0m         output \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(output)\n\u001b[0;32m    399\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    400\u001b[0m         \u001b[38;5;66;03m# evaluate only param, param_op combination separately and all x together\u001b[39;00m\n\u001b[0;32m    401\u001b[0m         \u001b[38;5;66;03m# Faster evaluation for lower-order derivatives\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\Qiskit\\squlearnRF\\src\\squlearn\\qnn\\lowlevel_qnn_pennylane.py:390\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    382\u001b[0m \n\u001b[0;32m    383\u001b[0m     \u001b[38;5;66;03m# Direct evaluation of the QNN\u001b[39;00m\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (todo_class\u001b[38;5;241m.\u001b[39mreturn_grad_x \u001b[38;5;129;01mand\u001b[39;00m todo_class\u001b[38;5;241m.\u001b[39morder \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m    385\u001b[0m         todo_class\u001b[38;5;241m.\u001b[39morder \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor\u001b[38;5;241m.\u001b[39mshots \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    386\u001b[0m     ):  \u001b[38;5;66;03m# TODO: Should be removed if PennyLane bug 4462 is fixed\u001b[39;00m\n\u001b[0;32m    387\u001b[0m         \u001b[38;5;66;03m# evaluate every single x, param, param_op combination separately\u001b[39;00m\n\u001b[0;32m    388\u001b[0m         \u001b[38;5;66;03m# faster evaluation for higher-order derivatives w.r.t. x\u001b[39;00m\n\u001b[0;32m    389\u001b[0m         output \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 390\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate_todo_single_x\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtodo_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_inp_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_inp_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_obs_inp_\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    393\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m x_inp_ \u001b[38;5;129;01min\u001b[39;00m x_inp\n\u001b[0;32m    394\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m param_inp_ \u001b[38;5;129;01min\u001b[39;00m param_inp\n\u001b[0;32m    395\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m param_obs_inp_ \u001b[38;5;129;01min\u001b[39;00m param_obs_inp\n\u001b[0;32m    396\u001b[0m         ]\n\u001b[0;32m    397\u001b[0m         output \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(output)\n\u001b[0;32m    399\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    400\u001b[0m         \u001b[38;5;66;03m# evaluate only param, param_op combination separately and all x together\u001b[39;00m\n\u001b[0;32m    401\u001b[0m         \u001b[38;5;66;03m# Faster evaluation for lower-order derivatives\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\Qiskit\\squlearnRF\\src\\squlearn\\qnn\\lowlevel_qnn_pennylane.py:536\u001b[0m, in \u001b[0;36mLowLevelQNNPennyLane._evaluate_todo_single_x\u001b[1;34m(self, todo_class, x, param, param_obs)\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    534\u001b[0m         deriv \u001b[38;5;241m=\u001b[39m qml\u001b[38;5;241m.\u001b[39mjacobian(deriv, argnum\u001b[38;5;241m=\u001b[39marg_index)\n\u001b[1;32m--> 536\u001b[0m hash_value \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhash\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(todo_class\u001b[38;5;241m.\u001b[39margnum)\n\u001b[0;32m    537\u001b[0m deriv\u001b[38;5;241m.\u001b[39mhash \u001b[38;5;241m=\u001b[39m hash_value\n\u001b[0;32m    538\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor\u001b[38;5;241m.\u001b[39mpennylane_execute(deriv, \u001b[38;5;241m*\u001b[39m(eval_tuple))\n",
      "File \u001b[1;32m~\\Desktop\\Qiskit\\squlearnRF\\src\\squlearn\\util\\pennylane\\pennylane_circuit.py:203\u001b[0m, in \u001b[0;36mPennyLaneCircuit.hash\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhash\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    202\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Hashable object of the circuit and observable for caching\"\"\"\u001b[39;00m\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_qiskit_circuit\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qiskit_observable)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\qiskit\\circuit\\quantumcircuit.py:491\u001b[0m, in \u001b[0;36mQuantumCircuit.__str__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m--> 491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\qiskit\\circuit\\quantumcircuit.py:1807\u001b[0m, in \u001b[0;36mQuantumCircuit.draw\u001b[1;34m(self, output, scale, filename, style, interactive, plot_barriers, reverse_bits, justify, vertical_compression, idle_wires, with_layout, fold, ax, initial_state, cregbundle, wire_order, expr_len)\u001b[0m\n\u001b[0;32m   1804\u001b[0m \u001b[38;5;66;03m# pylint: disable=cyclic-import\u001b[39;00m\n\u001b[0;32m   1805\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqiskit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvisualization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m circuit_drawer\n\u001b[1;32m-> 1807\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcircuit_drawer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1808\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1809\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1811\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstyle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstyle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1812\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1813\u001b[0m \u001b[43m    \u001b[49m\u001b[43minteractive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minteractive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplot_barriers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplot_barriers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreverse_bits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreverse_bits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjustify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjustify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1817\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvertical_compression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvertical_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1818\u001b[0m \u001b[43m    \u001b[49m\u001b[43midle_wires\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midle_wires\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1819\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_layout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_layout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1820\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1821\u001b[0m \u001b[43m    \u001b[49m\u001b[43max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1822\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcregbundle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcregbundle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1824\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwire_order\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwire_order\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1825\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpr_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpr_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1826\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\qiskit\\visualization\\circuit\\circuit_visualization.py:197\u001b[0m, in \u001b[0;36mcircuit_drawer\u001b[1;34m(circuit, scale, filename, style, output, interactive, plot_barriers, reverse_bits, justify, vertical_compression, idle_wires, with_layout, fold, ax, initial_state, cregbundle, wire_order, expr_len)\u001b[0m\n\u001b[0;32m    195\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    196\u001b[0m expr_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(expr_len, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 197\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43muser_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# Get default from config file else use text\u001b[39;00m\n\u001b[0;32m    199\u001b[0m default_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\qiskit\\user_config.py:235\u001b[0m, in \u001b[0;36mget_config\u001b[1;34m()\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Read the config file from the default location or env var\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \n\u001b[0;32m    227\u001b[0m \u001b[38;5;124;03mIt will read a config file at either the default location\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;124;03m    dict: The settings dict from the parsed config file.\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    234\u001b[0m filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQISKIT_SETTINGS\u001b[39m\u001b[38;5;124m\"\u001b[39m, DEFAULT_FILENAME)\n\u001b[1;32m--> 235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[0;32m    237\u001b[0m user_config \u001b[38;5;241m=\u001b[39m UserConfig(filename)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\genericpath.py:30\u001b[0m, in \u001b[0;36misfile\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Test whether a path is a regular file\"\"\"\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     st \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "y = np.zeros((input_values.shape[0]))\n",
    "clf._fit(input_values, y,  weights=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdp shape:  (12, 16)\n",
    "dfdpdx shape:  (12, 16, 1)\n",
    "dfdpdxdx shape:  (12, 16, 1, 1)\n",
    "\n",
    "\n",
    "\n",
    "dfdp shape:  (12, 16)\n",
    "dfdxdp shape:  (12, 1, 16)\n",
    "dfdxdxdp shape:  (12, 1, 1, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.15719103, -0.03485702,  0.09964409,  0.22748242,  0.33785641,\n",
       "        0.42799301,  0.49998125])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_ODE.predict(input_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<squlearn.qnn.loss.ODELoss at 0x1f0178450f0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_ODE.loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1f01430b850>]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(input_values, clf_ODE.predict(input_values))\n",
    "plt.plot(input_values, np.sin(input_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from squlearn.qnn.base_qnn import QNN\n",
    "num_qubits = 1\n",
    "num_features = 1\n",
    "num_layers = 3 \n",
    "\n",
    "\n",
    "circuit = ChebyshevRx(num_qubits, num_features, num_layers)\n",
    "observable = SummedPaulis(num_qubits) #observable.num_parameters = num_qubits+1\n",
    "param_ini = np.random.rand(2*num_qubits*num_layers)\n",
    "executor = Executor(\"statevector_simulator\")\n",
    "\n",
    "qnn_ = QNN(circuit, observable, executor, result_caching=False, optree_caching=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'param_op_ini' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m input_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([  [\u001b[38;5;241m0.1\u001b[39m],[\u001b[38;5;241m0.2\u001b[39m]])\n\u001b[1;32m----> 2\u001b[0m loss_values \u001b[38;5;241m=\u001b[39m qnn_\u001b[38;5;241m.\u001b[39mevaluate((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdfdx\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdfdxdx\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdfdp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdfdpdx\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdfdpdxdx\u001b[39m\u001b[38;5;124m\"\u001b[39m), input_values, param_ini, \u001b[43mparam_op_ini\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'param_op_ini' is not defined"
     ]
    }
   ],
   "source": [
    "input_values = np.array([  [0.1],[0.2]])\n",
    "loss_values = qnn_.evaluate((\"f\", \"dfdx\", \"dfdxdx\", \"dfdp\", \"dfdpdx\", \"dfdpdxdx\"), input_values, param_ini, param_op_ini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.19317778, 0.22126637])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_values[\"dfdxdx\"][:,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.16367608, -0.14300414])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.40807955, 0.39272217]],\n",
       "\n",
       "       [[0.43616814, 0.42081076]]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_values[\"f\"]+loss_values[\"dfdxdx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f': array([[0.40807955, 0.39272217],\n",
       "        [0.43616814, 0.42081076]])}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_functional(loss_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_values[\"dfdp\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_functional(loss_values, x_span):\n",
    "    f = loss_values[\"f\"]\n",
    "    dfdx = loss_values[\"dfdx\"] \n",
    "    dfdxdx = loss_values[\"dfdxdx\"]\n",
    "    value = dfdxdx + f #design your L functional here\n",
    "\n",
    "    return {\"f\": value}\n",
    "\n",
    "def grad_L_functional(loss_values, x_span):\n",
    "    \"\"\"ArithmeticError\n",
    "    n = x_span.shape[0] number of points\n",
    "    m = x_span.shape[1] number of dimensions (typically m=1)\n",
    "\n",
    "    F[x, x_, x__] = envelope(x, x_, x__)\n",
    "\n",
    "    grad_envelope = (envelope(x, x_, x__)dx, envelope(x, x_, x__)dx_, envelope(x, x_, x__)dx__)\n",
    "\n",
    "    \"\"\"\n",
    "    f = loss_values[\"f\"] # shape (n,)\n",
    "    dfdx = loss_values[\"dfdx\"] # shape (n, m) \n",
    "    dfdxdx = loss_values[\"dfdxdx\"][:,0,:] # shape (n, 1, m)\n",
    "\n",
    "    dfdp = loss_values[\"dfdp\"] # shape (n, p)\n",
    "    n_param = dfdp.shape[1]\n",
    "    dfdpdx = loss_values[\"dfdpdx\"][:,0,:] # shape (n, 1, p)\n",
    "    dfdpdxdx = loss_values[\"dfdpdxdx\"][:,0,0,:] # shape (n, 1, 1, p)\n",
    "    \n",
    "    #problem dependent gradient!!\n",
    "    grad_envelope_list = np.zeros((3, x_span.shape[0], n_param)) # shape (3, n, p) \n",
    "    grad_envelope_list[0,:,:] = 1\n",
    "    grad_envelope_list[1,:,:] = 0\n",
    "    grad_envelope_list[2,:,:] = 1\n",
    "    ###########3\n",
    "\n",
    "\n",
    "    grad_p = np.array([dfdp, dfdpdx, dfdpdxdx])  # shape (n, p), (n, p), (n, p) -> (3, n, p)\n",
    "    \n",
    "    total_grad_einsum = np.einsum(\"ijk,ijk->jk\", grad_envelope_list, grad_p) \n",
    "    #same as below\n",
    "    #for n_i in range(x_span.shape[0]):\n",
    "    #    for p_i in range(n_param):\n",
    "    #        total_grad_[n_i, p_i] = np.sum(grad_envelope_list[:,n_i,p_i] * grad_p[:,n_i,p_i])\n",
    "    #print(np.array_equal(total_grad_, total_grad_einsum))\n",
    "\n",
    "\n",
    "    return {\"dfdx\": total_grad_einsum}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print squl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_qubits = 6\n",
    "num_features = 1\n",
    "num_layers = 3\n",
    "\n",
    "\n",
    "circuit = ChebyshevRx(num_qubits, num_features, num_layers)\n",
    "observable = SummedPaulis(num_qubits) #observable.num_parameters = num_qubits+1\n",
    "param_ini = np.random.rand(2*num_qubits*num_layers)\n",
    "param_op_ini = np.random.rand(num_qubits+1)\n",
    "executor = Executor(\"statevector_simulator\")\n",
    "\n",
    "qnn_ = QNN(circuit, observable, executor, result_caching=False, optree_caching=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loss(optimization_parameters, X_train, qnn_, problem_parameters):\n",
    "    param_ini, param_op_ini = optimization_parameters[:qnn_.num_parameters], optimization_parameters[qnn_.num_parameters:]\n",
    "    g, f_initial, regularization_parameter = problem_parameters\n",
    "    output_f = qnn_.evaluate(\"f\", X_train, param_ini, param_op_ini)[\"f\"]\n",
    "    output_dfdx = qnn_.evaluate(\"dfdx\", X_train, param_ini, param_op_ini)[\"dfdx\"]\n",
    "    return loss_function(output_f, output_dfdx, X_train, g, f_initial, regularization_parameter)\n",
    "\n",
    "def loss_function(f_0, f_1, X, g, f_initial, regularization_parameter):\n",
    "        \"\"\"Calculates the loss function.\n",
    "\n",
    "        f_0 = zeroth order derivative\n",
    "        f_1 = first order derivative\n",
    "        g = differential equation function\n",
    "        Args:\n",
    "            alpha_ (np.ndarray): The vector of alphas, of shape (len(x_span)+1, \n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        sum1 = np.sum( (f_1 - g(f_0, X))**2 )\n",
    "        sum2 = (f_0[0] - f_initial)**2\n",
    "        L = sum2 + sum1 * regularization_parameter\n",
    "        print(L)\n",
    "        return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_paper(f, x):\n",
    "        \"\"\"\n",
    "        df/dx = -lamb * np.exp(-lamb * x * k) * np.sin(lamb * x) - lamb * k * f\n",
    "\n",
    "        solution: f(x) = np.exp(-lamb * x * k) * np.cos(lamb * x), f(0) = 1\n",
    "        \"\"\"\n",
    "        lamb = 20\n",
    "        k = 0.1\n",
    "        return -lamb * np.exp(-lamb * x * k) * np.sin(lamb * x) - lamb * k * f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_span = np.linspace(0.1, 0.9, 20)\n",
    "f_initial = 1\n",
    "g = g_paper\n",
    "\n",
    "from scipy.integrate import odeint\n",
    "from scipy.optimize import minimize\n",
    "#Numerical and Analytical Solutions\n",
    "f_odeint = odeint(g, f_initial, x_span[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x181fb78ca60>]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_span, f_odeint, label=\"Odeint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18394.18050917232\n",
      "18394.180494279433\n",
      "18394.180509984133\n",
      "18394.180504239055\n",
      "18394.180522131628\n",
      "18394.180507536854\n",
      "18394.18050878806\n",
      "18394.180478577426\n",
      "18394.180509196147\n",
      "18394.18049899365\n",
      "18394.180526469278\n",
      "18394.180503777498\n",
      "18394.180508318655\n",
      "18394.180501620485\n",
      "18394.180510637536\n",
      "18394.18050716893\n",
      "18394.180509369082\n",
      "18394.180513275838\n",
      "18394.180508788064\n",
      "18394.180494475408\n",
      "18394.180510612347\n",
      "18394.180506111603\n",
      "18394.180509699894\n",
      "18394.18051198598\n",
      "18394.180508318652\n",
      "18394.180512448434\n",
      "18394.18050630663\n",
      "18394.180509803475\n",
      "18394.180512501563\n",
      "18394.180507536854\n",
      "18394.180508788064\n",
      "18394.18051370344\n",
      "18394.180500103295\n",
      "18394.180511245504\n",
      "18394.180511627805\n",
      "18394.180503777498\n",
      "18394.180508318652\n",
      "18394.180556003623\n",
      "18394.180519989266\n",
      "18394.18052168439\n",
      "18394.180520591053\n",
      "18394.180517967347\n",
      "18394.180515427503\n",
      "18394.180509932146\n",
      "17263.684077379752\n",
      "17263.68407847517\n",
      "17263.684075531393\n",
      "17263.684080740833\n",
      "17263.684070105\n",
      "17263.68407850352\n",
      "17263.684078002698\n",
      "17263.684079021637\n",
      "17263.68407519794\n",
      "17263.684080742052\n",
      "17263.684069161973\n",
      "17263.68407864147\n",
      "17263.68407809219\n",
      "17263.6840819549\n",
      "17263.684078137383\n",
      "17263.684079572857\n",
      "17263.684076951573\n",
      "17263.684073113724\n",
      "17263.684078002698\n",
      "17263.68407727572\n",
      "17263.68407877015\n",
      "17263.6840789191\n",
      "17263.684076952424\n",
      "17263.684071740547\n",
      "17263.68407809219\n",
      "17263.684071228217\n",
      "17263.684077719434\n",
      "17263.68407748667\n",
      "17263.68407616424\n",
      "17263.68407850352\n",
      "17263.684078002698\n",
      "17263.68407147283\n",
      "17263.68407793229\n",
      "17263.684077687747\n",
      "17263.684076172456\n",
      "17263.68407864147\n",
      "17263.68407809219\n",
      "17263.684061504497\n",
      "17263.68408494663\n",
      "17263.684077983133\n",
      "17263.684075723904\n",
      "17263.684077427282\n",
      "17263.684081839056\n",
      "17263.684078193302\n",
      "16867.78157376854\n",
      "16867.781572785534\n",
      "16867.781573687815\n",
      "16867.781573910786\n",
      "16867.7815737516\n",
      "16867.781573828684\n",
      "16867.781573808352\n",
      "16867.781573185348\n",
      "16867.781573712964\n",
      "16867.78157392355\n",
      "16867.781573773373\n",
      "16867.781573787855\n",
      "16867.78157382904\n",
      "16867.781574482295\n",
      "16867.78157368283\n",
      "16867.7815736611\n",
      "16867.781573825\n",
      "16867.78157359661\n",
      "16867.781573808352\n",
      "16867.781574261397\n",
      "16867.7815737029\n",
      "16867.781573671236\n",
      "16867.7815738389\n",
      "16867.781573655437\n",
      "16867.78157382904\n",
      "16867.781574614066\n",
      "16867.781572276206\n",
      "16867.781573942313\n",
      "16867.78157345427\n",
      "16867.781573828684\n",
      "16867.781573808352\n",
      "16867.781574203804\n",
      "16867.781572998043\n",
      "16867.781573849803\n",
      "16867.781573580156\n",
      "16867.781573787855\n",
      "16867.78157382904\n",
      "16867.781575885634\n",
      "16867.781573309876\n",
      "16867.781574371806\n",
      "16867.78157355001\n",
      "16867.781573917044\n",
      "16867.781574572196\n",
      "16867.781573895812\n",
      "17881.579815904086\n",
      "17881.579819762628\n",
      "17881.579797808685\n",
      "17881.579813896893\n",
      "17881.579818582446\n",
      "17881.57981511772\n",
      "17881.579816015626\n",
      "17881.57982312679\n",
      "17881.579769872955\n",
      "17881.579814734185\n",
      "17881.579816215773\n",
      "17881.57980803141\n",
      "17881.57981604389\n",
      "17881.579817895417\n",
      "17881.579810787032\n",
      "17881.57981710073\n",
      "17881.57981595232\n",
      "17881.57982718992\n",
      "17881.579816015626\n",
      "17881.579824865647\n",
      "17881.579814280496\n",
      "17881.579818890736\n",
      "17881.57981601776\n",
      "17881.57983438941\n",
      "17881.579816043886\n",
      "17881.57980340228\n",
      "17881.57981472607\n",
      "17881.579814908167\n",
      "17881.579817338406\n",
      "17881.57981511772\n",
      "17881.579816015626\n",
      "17881.579799618983\n",
      "17881.579816305457\n",
      "17881.579811056716\n",
      "17881.579814099077\n",
      "17881.579808031405\n",
      "17881.57981604389\n",
      "17881.57977836991\n",
      "17881.579851356855\n",
      "17881.579805595327\n",
      "17881.579815766658\n",
      "17881.57981573588\n",
      "17881.579825940757\n",
      "17881.579818469672\n",
      "16819.67730649258\n",
      "16819.67730856914\n",
      "16819.677308022692\n",
      "16819.67730708858\n",
      "16819.677307226088\n",
      "16819.677307399572\n",
      "16819.677306413687\n",
      "16819.677307165657\n",
      "16819.677307070197\n",
      "16819.677306583722\n",
      "16819.677306803715\n",
      "16819.67730699898\n",
      "16819.67730638981\n",
      "16819.677305980254\n",
      "16819.677307517497\n",
      "16819.677306657424\n",
      "16819.677306236423\n",
      "16819.67730761199\n",
      "16819.677306413687\n",
      "16819.677307235426\n",
      "16819.67730782848\n",
      "16819.677306618676\n",
      "16819.67730631563\n",
      "16819.677307247795\n",
      "16819.67730638981\n",
      "16819.677308557068\n",
      "16819.677308423634\n",
      "16819.677306301048\n",
      "16819.67730683696\n",
      "16819.677307399572\n",
      "16819.677306413687\n",
      "16819.67730842256\n",
      "16819.67730924621\n",
      "16819.67730539111\n",
      "16819.677306333288\n",
      "16819.67730699898\n",
      "16819.67730638981\n",
      "16819.677301020485\n",
      "16819.677308269085\n",
      "16819.67730655315\n",
      "16819.677306139347\n",
      "16819.6773055085\n",
      "16819.677306330563\n",
      "16819.67730727626\n",
      "16784.79547883203\n",
      "16784.795478738877\n",
      "16784.79547913281\n",
      "16784.79547965575\n",
      "16784.795478925484\n",
      "16784.79547889755\n",
      "16784.795478797172\n",
      "16784.79547864901\n",
      "16784.795478958327\n",
      "16784.79547935443\n",
      "16784.795479016306\n",
      "16784.795478880547\n",
      "16784.795478792308\n",
      "16784.79547883989\n",
      "16784.79547868664\n",
      "16784.795478840344\n",
      "16784.795478848715\n",
      "16784.795478980734\n",
      "16784.795478797172\n",
      "16784.795479266148\n",
      "16784.795478922206\n",
      "16784.795478743352\n",
      "16784.7954788721\n",
      "16784.79547893026\n",
      "16784.795478792304\n",
      "16784.79547947239\n",
      "16784.795477756994\n",
      "16784.79547898354\n",
      "16784.795478624455\n",
      "16784.79547889755\n",
      "16784.795478797172\n",
      "16784.795479521523\n",
      "16784.795478665288\n",
      "16784.795478689957\n",
      "16784.795478652046\n",
      "16784.795478880547\n",
      "16784.795478792304\n",
      "16784.79547850039\n",
      "16784.795479379674\n",
      "16784.795480491182\n",
      "16784.795477919364\n",
      "16784.79547865295\n",
      "16784.795479466386\n",
      "16784.79547911321\n",
      "16852.350702324362\n",
      "16852.350705528854\n",
      "16852.350706158646\n",
      "16852.350701061314\n",
      "16852.350702784173\n",
      "16852.35070347247\n",
      "16852.350702208794\n",
      "16852.35070298627\n",
      "16852.350706247675\n",
      "16852.350701810403\n",
      "16852.350702655338\n",
      "16852.350702961467\n",
      "16852.350702166048\n",
      "16852.35070116111\n",
      "16852.350706437934\n",
      "16852.35070250478\n",
      "16852.350702251802\n",
      "16852.350702946056\n",
      "16852.350702208794\n",
      "16852.350699768456\n",
      "16852.350705613546\n",
      "16852.35070177506\n",
      "16852.350702188196\n",
      "16852.350702391097\n",
      "16852.350702166048\n",
      "16852.35070572632\n",
      "16852.350701539148\n",
      "16852.3507025279\n",
      "16852.35070393037\n",
      "16852.35070347247\n",
      "16852.350702208794\n",
      "16852.35070318194\n",
      "16852.350700025512\n",
      "16852.350702609972\n",
      "16852.350703571192\n",
      "16852.350702961463\n",
      "16852.35070216605\n",
      "16852.350707097274\n",
      "16852.35069868283\n",
      "16852.35069929857\n",
      "16852.350702956584\n",
      "16852.350702706543\n",
      "16852.35069918905\n",
      "16852.35070139786\n",
      "16755.997709406307\n",
      "16755.997709912186\n",
      "16755.997710143707\n",
      "16755.997709720752\n",
      "16755.997709557512\n",
      "16755.99770969473\n",
      "16755.997709319567\n",
      "16755.99770945297\n",
      "16755.997709893993\n",
      "16755.99770963567\n",
      "16755.997709551644\n",
      "16755.997709615927\n",
      "16755.99770929894\n",
      "16755.997708543888\n",
      "16755.997709744297\n",
      "16755.99770946033\n",
      "16755.997709355703\n",
      "16755.9977096793\n",
      "16755.997709319567\n",
      "16755.997708884996\n",
      "16755.997709872605\n",
      "16755.997709300835\n",
      "16755.997709364052\n",
      "16755.997709567695\n",
      "16755.99770929894\n",
      "16755.99770953067\n",
      "16755.99770890099\n",
      "16755.99770970167\n",
      "16755.997709597883\n",
      "16755.99770969473\n",
      "16755.997709319567\n",
      "16755.997709471296\n",
      "16755.997709092982\n",
      "16755.997709414965\n",
      "16755.997709543597\n",
      "16755.997709615927\n",
      "16755.99770929894\n",
      "16755.997711363292\n",
      "16755.997709536052\n",
      "16755.997709875686\n",
      "16755.997708791925\n",
      "16755.99770928984\n",
      "16755.997709141873\n",
      "16755.99770934007\n",
      "16746.805564884362\n",
      "16746.80556575698\n",
      "16746.805565680203\n",
      "16746.805565178267\n",
      "16746.805565229424\n",
      "16746.805565257924\n",
      "16746.805564827195\n",
      "16746.805565078696\n",
      "16746.80556525274\n",
      "16746.805564902323\n",
      "16746.805565077182\n",
      "16746.805565157432\n",
      "16746.805564769977\n",
      "16746.805565108196\n",
      "16746.80556526144\n",
      "16746.805565059352\n",
      "16746.80556481079\n",
      "16746.805565412586\n",
      "16746.805564827195\n",
      "16746.805566062107\n",
      "16746.805565675706\n",
      "16746.805564988714\n",
      "16746.80556482947\n",
      "16746.805565291925\n",
      "16746.80556476998\n",
      "16746.805566186373\n",
      "16746.805566846837\n",
      "16746.8055647357\n",
      "16746.80556490296\n",
      "16746.805565257924\n",
      "16746.805564827195\n",
      "16746.80556635714\n",
      "16746.805568199354\n",
      "16746.805563507547\n",
      "16746.805564695154\n",
      "16746.805565157432\n",
      "16746.80556476998\n",
      "16746.805559317207\n",
      "16746.80556628832\n",
      "16746.80556552081\n",
      "16746.805564547118\n",
      "16746.80556419767\n",
      "16746.80556562833\n",
      "16746.80556554842\n",
      "16737.122446801863\n",
      "16737.122447486727\n",
      "16737.122447676087\n",
      "16737.122447097114\n",
      "16737.122447028774\n",
      "16737.122447144677\n",
      "16737.12244672719\n",
      "16737.12244685523\n",
      "16737.122447315425\n",
      "16737.12244699737\n",
      "16737.122446979614\n",
      "16737.122447053112\n",
      "16737.12244668944\n",
      "16737.122446240322\n",
      "16737.122447308073\n",
      "16737.12244686605\n",
      "16737.122446721554\n",
      "16737.122447182766\n",
      "16737.12244672719\n",
      "16737.122446836307\n",
      "16737.12244752544\n",
      "16737.12244668748\n",
      "16737.122446732123\n",
      "16737.12244705314\n",
      "16737.12244668944\n",
      "16737.122447407477\n",
      "16737.122447106816\n",
      "16737.12244699664\n",
      "16737.122446952362\n",
      "16737.122447144677\n",
      "16737.12244672719\n",
      "16737.122447443708\n",
      "16737.122447709557\n",
      "16737.122446369638\n",
      "16737.12244682657\n",
      "16737.122447053112\n",
      "16737.12244668944\n",
      "16737.12244559694\n",
      "16737.122447448604\n",
      "16737.122447251768\n",
      "16737.122446154717\n",
      "16737.122446433186\n",
      "16737.122446903933\n",
      "16737.122446944573\n",
      "16708.464370723577\n",
      "16708.464370985348\n",
      "16708.464370583668\n",
      "16708.464371372018\n",
      "16708.464371029928\n",
      "16708.464370971513\n",
      "16708.46437068911\n",
      "16708.464370663452\n",
      "16708.4643707072\n",
      "16708.464371032373\n",
      "16708.46437082655\n",
      "16708.464370885522\n",
      "16708.46437064965\n",
      "16708.4643708274\n",
      "16708.464370223923\n",
      "16708.464370600566\n",
      "16708.4643706672\n",
      "16708.46437103933\n",
      "16708.464370689115\n",
      "16708.464371521066\n",
      "16708.46437085666\n",
      "16708.464370665217\n",
      "16708.464370680846\n",
      "16708.46437101997\n",
      "16708.46437064965\n",
      "16708.464370952563\n",
      "16708.46437147741\n",
      "16708.464370748978\n",
      "16708.464370718506\n",
      "16708.464370971516\n",
      "16708.464370689115\n",
      "16708.464371322876\n",
      "16708.464372441667\n",
      "16708.464370033544\n",
      "16708.464370582573\n",
      "16708.464370885522\n",
      "16708.46437064965\n",
      "16708.464368342942\n",
      "16708.464371593003\n",
      "16708.464371789887\n",
      "16708.464369943304\n",
      "16708.464370338002\n",
      "16708.46437136864\n",
      "16708.46437109604\n",
      "16697.676822998517\n",
      "16697.676823502938\n",
      "16697.676823322377\n",
      "16697.676822609203\n",
      "16697.676823481328\n",
      "16697.676823282713\n",
      "16697.676823034562\n",
      "16697.676823593054\n",
      "16697.676823565067\n",
      "16697.67682250926\n",
      "16697.676823673388\n",
      "16697.676823330978\n",
      "16697.676823070666\n",
      "16697.676821523757\n",
      "16697.676822596415\n",
      "16697.676822812908\n",
      "16697.676822948833\n",
      "16697.676823064685\n",
      "16697.676823034562\n",
      "16697.676820069526\n",
      "16697.67682284316\n",
      "16697.676822554888\n",
      "16697.676822912945\n",
      "16697.676822800408\n",
      "16697.676823070666\n",
      "16697.67682273898\n",
      "16697.676821952497\n",
      "16697.676823768652\n",
      "16697.676823420024\n",
      "16697.676823282716\n",
      "16697.676823034562\n",
      "16697.676822090318\n",
      "16697.676819777666\n",
      "16697.67682478254\n",
      "16697.676823583868\n",
      "16697.676823330978\n",
      "16697.676823070666\n",
      "16697.676827769297\n",
      "16697.676820609693\n",
      "16697.676822647576\n",
      "16697.676824243306\n",
      "16697.67682350578\n",
      "16697.67682245021\n",
      "16697.67682168635\n",
      "16688.055363188472\n",
      "16688.05536343546\n",
      "16688.055363122323\n",
      "16688.05536344645\n",
      "16688.055363472533\n",
      "16688.0553634055\n",
      "16688.055363173517\n",
      "16688.055363210147\n",
      "16688.055363385625\n",
      "16688.055363300908\n",
      "16688.055363311076\n",
      "16688.055363322717\n",
      "16688.05536315369\n",
      "16688.055362834162\n",
      "16688.055362695428\n",
      "16688.055363066556\n",
      "16688.055363126994\n",
      "16688.05536335835\n",
      "16688.05536317352\n",
      "16688.05536298994\n",
      "16688.055363241296\n",
      "16688.055363068303\n",
      "16688.05536311468\n",
      "16688.055363377687\n",
      "16688.05536315369\n",
      "16688.055363198553\n",
      "16688.05536341258\n",
      "16688.055363340507\n",
      "16688.055363335952\n",
      "16688.0553634055\n",
      "16688.05536317352\n",
      "16688.055363366908\n",
      "16688.05536361267\n",
      "16688.055363024356\n",
      "16688.05536325072\n",
      "16688.055363322714\n",
      "16688.05536315369\n",
      "16688.05536308477\n",
      "16688.055363233438\n",
      "16688.055363889398\n",
      "16688.05536284198\n",
      "16688.05536306922\n",
      "16688.055363524858\n",
      "16688.05536302121\n",
      "16663.13905520365\n",
      "16663.139055073574\n",
      "16663.13905397005\n",
      "16663.13905565396\n",
      "16663.139055133677\n",
      "16663.139055185584\n",
      "16663.139055159194\n",
      "16663.13905522246\n",
      "16663.13905450476\n",
      "16663.139055392356\n",
      "16663.139055137875\n",
      "16663.139055214855\n",
      "16663.139055166565\n",
      "16663.139056028063\n",
      "16663.139053792922\n",
      "16663.139055064436\n",
      "16663.13905528423\n",
      "16663.139054348932\n",
      "16663.139055159194\n",
      "16663.139056225446\n",
      "16663.139054402516\n",
      "16663.139055154967\n",
      "16663.139055308497\n",
      "16663.13905487082\n",
      "16663.139055166565\n",
      "16663.13905433886\n",
      "16663.139056243024\n",
      "16663.139054691626\n",
      "16663.139055184856\n",
      "16663.139055185584\n",
      "16663.139055159194\n",
      "16663.139054919437\n",
      "16663.13905680314\n",
      "16663.139054551448\n",
      "16663.1390551545\n",
      "16663.139055214855\n",
      "16663.139055166568\n",
      "16663.13905362878\n",
      "16663.139055779196\n",
      "16663.13905557273\n",
      "16663.139054630883\n",
      "16663.139055166863\n",
      "16663.139055867276\n",
      "16663.139055663243\n",
      "16667.794654454257\n",
      "16667.794654402318\n",
      "16667.794654380366\n",
      "16667.79465427004\n",
      "16667.794654174275\n",
      "16667.79465381124\n",
      "16667.7946545291\n",
      "16667.794653776364\n",
      "16667.794654556696\n",
      "16667.79465430457\n",
      "16667.794654086654\n",
      "16667.79465399174\n",
      "16667.79465448208\n",
      "16667.794654615933\n",
      "16667.794654016863\n",
      "16667.794654865414\n",
      "16667.794654432168\n",
      "16667.794654776193\n",
      "16667.7946545291\n",
      "16667.794654494268\n",
      "16667.794653821496\n",
      "16667.79465471273\n",
      "16667.7946543572\n",
      "16667.794654871515\n",
      "16667.79465448208\n",
      "16667.794654446025\n",
      "16667.79465454656\n",
      "16667.794654885427\n",
      "16667.794654250476\n",
      "16667.79465381124\n",
      "16667.7946545291\n",
      "16667.79465466431\n",
      "16667.79465368028\n",
      "16667.794654774978\n",
      "16667.79465422806\n",
      "16667.79465399174\n",
      "16667.79465448208\n",
      "16667.79465452516\n",
      "16667.794654110545\n",
      "16667.794654117584\n",
      "16667.79465474861\n",
      "16667.794654435747\n",
      "16667.794654514004\n",
      "16667.79465443368\n",
      "16651.780394396555\n",
      "16651.78039460457\n",
      "16651.780394660927\n",
      "16651.780394246103\n",
      "16651.780394519566\n",
      "16651.780394417394\n",
      "16651.780394416834\n",
      "16651.780394589223\n",
      "16651.780394709378\n",
      "16651.78039426232\n",
      "16651.780394524558\n",
      "16651.780394437446\n",
      "16651.780394423044\n",
      "16651.780394334222\n",
      "16651.780394445534\n",
      "16651.780394467092\n",
      "16651.780394362962\n",
      "16651.78039487828\n",
      "16651.780394416837\n",
      "16651.780394130477\n",
      "16651.78039456196\n",
      "16651.78039444238\n",
      "16651.780394353926\n",
      "16651.780394785023\n",
      "16651.780394423044\n",
      "16651.780394653328\n",
      "16651.78039450072\n",
      "16651.78039450146\n",
      "16651.780394500413\n",
      "16651.780394417394\n",
      "16651.780394416834\n",
      "16651.78039465772\n",
      "16651.78039404864\n",
      "16651.780394578593\n",
      "16651.780394489386\n",
      "16651.780394437446\n",
      "16651.780394423044\n",
      "16651.780394765385\n",
      "16651.780394158417\n",
      "16651.780394266825\n",
      "16651.780394702393\n",
      "16651.780394437017\n",
      "16651.780394484285\n",
      "16651.780394254543\n",
      "16793.411785663597\n",
      "16793.41178085509\n",
      "16793.41178255309\n",
      "16793.411786287048\n",
      "16793.41178456643\n",
      "16793.41178294344\n",
      "16793.411785484077\n",
      "16793.41177919854\n",
      "16793.411782873805\n",
      "16793.411786651013\n",
      "16793.4117834848\n",
      "16793.411783443436\n",
      "16793.411785099364\n",
      "16793.41178567295\n",
      "16793.411785030807\n",
      "16793.41178634218\n",
      "16793.411785572607\n",
      "16793.411783733813\n",
      "16793.411785484077\n",
      "16793.411789597143\n",
      "16793.411782814794\n",
      "16793.411786341923\n",
      "16793.411785213197\n",
      "16793.41178585058\n",
      "16793.41178509936\n",
      "16793.411782695686\n",
      "16793.411787741647\n",
      "16793.411786440607\n",
      "16793.41178451659\n",
      "16793.411782943436\n",
      "16793.411785484077\n",
      "16793.41178333976\n",
      "16793.41179247898\n",
      "16793.41178313292\n",
      "16793.41178435711\n",
      "16793.411783443436\n",
      "16793.411785099364\n",
      "16793.41177527293\n",
      "16793.411788724607\n",
      "16793.41178819402\n",
      "16793.411783289273\n",
      "16793.411783814045\n",
      "16793.411786272176\n",
      "16793.411786068\n",
      "16650.23354422366\n",
      "16650.233544270443\n",
      "16650.233544317896\n",
      "16650.23354414533\n",
      "16650.233544273677\n",
      "16650.233544152667\n",
      "16650.233544228755\n",
      "16650.233544129507\n",
      "16650.23354439033\n",
      "16650.23354419641\n",
      "16650.23354422107\n",
      "16650.23354417211\n",
      "16650.233544199567\n",
      "16650.233544189185\n",
      "16650.23354407299\n",
      "16650.233544260278\n",
      "16650.23354418834\n",
      "16650.233544564282\n",
      "16650.233544228755\n",
      "16650.23354426993\n",
      "16650.233544190207\n",
      "16650.23354425193\n",
      "16650.233544163206\n",
      "16650.23354459927\n",
      "16650.233544199567\n",
      "16650.233544320898\n",
      "16650.233544299244\n",
      "16650.23354428677\n",
      "16650.23354428367\n",
      "16650.233544152667\n",
      "16650.233544228755\n",
      "16650.23354437987\n",
      "16650.23354427441\n",
      "16650.233544135554\n",
      "16650.23354424149\n",
      "16650.233544172115\n",
      "16650.233544199567\n",
      "16650.233543926694\n",
      "16650.233544195762\n",
      "16650.233544390867\n",
      "16650.233544280614\n",
      "16650.233544170398\n",
      "16650.233544366638\n",
      "16650.23354415211\n",
      "16647.592033391622\n",
      "16647.5920333782\n",
      "16647.59203331973\n",
      "16647.592033377714\n",
      "16647.592033444784\n",
      "16647.59203331188\n",
      "16647.5920333909\n",
      "16647.59203330508\n",
      "16647.592033431967\n",
      "16647.592033390658\n",
      "16647.59203340614\n",
      "16647.59203333976\n",
      "16647.592033374076\n",
      "16647.592033318368\n",
      "16647.592033169767\n",
      "16647.592033387067\n",
      "16647.59203337143\n",
      "16647.592033539193\n",
      "16647.5920333909\n",
      "16647.592033404555\n",
      "16647.592033276887\n",
      "16647.592033395256\n",
      "16647.59203335208\n",
      "16647.592033611618\n",
      "16647.592033374076\n",
      "16647.59203332064\n",
      "16647.59203340432\n",
      "16647.59203348298\n",
      "16647.592033449044\n",
      "16647.592033311885\n",
      "16647.592033390905\n",
      "16647.59203342293\n",
      "16647.592033411624\n",
      "16647.592033360896\n",
      "16647.59203342345\n",
      "16647.59203333976\n",
      "16647.592033374076\n",
      "16647.592033229652\n",
      "16647.59203338001\n",
      "16647.59203352945\n",
      "16647.59203338052\n",
      "16647.59203335704\n",
      "16647.592033568657\n",
      "16647.59203335247\n",
      "16645.778409977418\n",
      "16645.778409903345\n",
      "16645.77840953246\n",
      "16645.7784101326\n",
      "16645.77840999575\n",
      "16645.778410029438\n",
      "16645.778409938335\n",
      "16645.77840990088\n",
      "16645.778409643604\n",
      "16645.778410134524\n",
      "16645.778409891573\n",
      "16645.778410074632\n",
      "16645.778409907518\n",
      "16645.778410354273\n",
      "16645.778409999646\n",
      "16645.778410020703\n",
      "16645.778409987728\n",
      "16645.778409777307\n",
      "16645.778409938335\n",
      "16645.778410754137\n",
      "16645.77841004231\n",
      "16645.778410055922\n",
      "16645.77841000564\n",
      "16645.778409972358\n",
      "16645.778409907518\n",
      "16645.77840977893\n",
      "16645.778410432205\n",
      "16645.778409654515\n",
      "16645.77841002424\n",
      "16645.778410029438\n",
      "16645.778409938335\n",
      "16645.77840989452\n",
      "16645.77841102483\n",
      "16645.778409295013\n",
      "16645.778409992672\n",
      "16645.778410074636\n",
      "16645.778409907518\n",
      "16645.778408699505\n",
      "16645.778410442275\n",
      "16645.778410291045\n",
      "16645.77840967016\n",
      "16645.778409832175\n",
      "16645.778410156534\n",
      "16645.778410128456\n",
      "16644.92777904747\n",
      "16644.927778552308\n",
      "16644.927778019315\n",
      "16644.92777934761\n",
      "16644.92777885503\n",
      "16644.927778839043\n",
      "16644.92777898638\n",
      "16644.927778789206\n",
      "16644.927778503934\n",
      "16644.927779204034\n",
      "16644.927778830664\n",
      "16644.927778937123\n",
      "16644.92777898094\n",
      "16644.927779568796\n",
      "16644.92777865015\n",
      "16644.927778888494\n",
      "16644.927779133246\n",
      "16644.927778065554\n",
      "16644.92777898638\n",
      "16644.927779657788\n",
      "16644.92777878008\n",
      "16644.927778965386\n",
      "16644.927779143527\n",
      "16644.92777858308\n",
      "16644.927778980942\n",
      "16644.92777855678\n",
      "16644.927779941714\n",
      "16644.927778616817\n",
      "16644.927778677007\n",
      "16644.927778839043\n",
      "16644.92777898638\n",
      "16644.927778803758\n",
      "16644.92778017207\n",
      "16644.927778487367\n",
      "16644.927778765334\n",
      "16644.927778937123\n",
      "16644.92777898094\n",
      "16644.9277778332\n",
      "16644.927779492667\n",
      "16644.927778985737\n",
      "16644.927778790054\n",
      "16644.927778964633\n",
      "16644.92777939615\n",
      "16644.927779364127\n",
      "16643.83037855172\n",
      "16643.83037815495\n",
      "16643.830377816405\n",
      "16643.830378762064\n",
      "16643.830378384573\n",
      "16643.83037843376\n",
      "16643.830378502123\n",
      "16643.83037834708\n",
      "16643.83037821769\n",
      "16643.830378628943\n",
      "16643.83037841303\n",
      "16643.83037847338\n",
      "16643.83037851046\n",
      "16643.830378969877\n",
      "16643.830378196926\n",
      "16643.83037840649\n",
      "16643.830378637096\n",
      "16643.83037776558\n",
      "16643.830378502123\n",
      "16643.8303788804\n",
      "16643.830378314422\n",
      "16643.830378490293\n",
      "16643.830378634244\n",
      "16643.830378144743\n",
      "16643.830378510458\n",
      "16643.830378178565\n",
      "16643.830379226278\n",
      "16643.83037831457\n",
      "16643.830378235773\n",
      "16643.83037843376\n",
      "16643.830378502123\n",
      "16643.83037836626\n",
      "16643.830379208688\n",
      "16643.830378371218\n",
      "16643.830378318667\n",
      "16643.83037847338\n",
      "16643.830378510458\n",
      "16643.83037789001\n",
      "16643.830378770235\n",
      "16643.8303783818\n",
      "16643.83037843024\n",
      "16643.830378543273\n",
      "16643.830378795923\n",
      "16643.830378821403\n",
      "16643.054692499463\n",
      "16643.0546923237\n",
      "16643.054691893936\n",
      "16643.054692661524\n",
      "16643.054692306345\n",
      "16643.054692378377\n",
      "16643.054692438614\n",
      "16643.0546926494\n",
      "16643.054692305126\n",
      "16643.054692522455\n",
      "16643.054692338224\n",
      "16643.054692457994\n",
      "16643.054692466274\n",
      "16643.054692751954\n",
      "16643.054692170364\n",
      "16643.05469226334\n",
      "16643.054692585913\n",
      "16643.054691896446\n",
      "16643.05469243861\n",
      "16643.05469251559\n",
      "16643.05469237969\n",
      "16643.054692353948\n",
      "16643.05469259592\n",
      "16643.054692100435\n",
      "16643.054692466274\n",
      "16643.05469225599\n",
      "16643.054693109196\n",
      "16643.054692040536\n",
      "16643.054692154692\n",
      "16643.054692378377\n",
      "16643.054692438614\n",
      "16643.054692376638\n",
      "16643.05469295464\n",
      "16643.05469220994\n",
      "16643.05469224611\n",
      "16643.054692457994\n",
      "16643.054692466274\n",
      "16643.05469218759\n",
      "16643.054692636866\n",
      "16643.054692233018\n",
      "16643.054692478232\n",
      "16643.054692562786\n",
      "16643.054692698348\n",
      "16643.05469272377\n",
      "16641.99751243372\n",
      "16641.99751226676\n",
      "16641.997511794576\n",
      "16641.99751262209\n",
      "16641.997512267226\n",
      "16641.997512255126\n",
      "16641.997512386948\n",
      "16641.997512541704\n",
      "16641.997512220343\n",
      "16641.997512493643\n",
      "16641.997512256898\n",
      "16641.997512343893\n",
      "16641.997512402264\n",
      "16641.997512600195\n",
      "16641.997512074002\n",
      "16641.997512249964\n",
      "16641.99751249128\n",
      "16641.99751176692\n",
      "16641.997512386944\n",
      "16641.997512443755\n",
      "16641.99751227567\n",
      "16641.997512338243\n",
      "16641.99751249651\n",
      "16641.99751199859\n",
      "16641.997512402264\n",
      "16641.997512214155\n",
      "16641.997512853643\n",
      "16641.997512061505\n",
      "16641.997512113358\n",
      "16641.997512255122\n",
      "16641.997512386944\n",
      "16641.99751230259\n",
      "16641.997512788414\n",
      "16641.997512131842\n",
      "16641.997512182927\n",
      "16641.99751234389\n",
      "16641.997512402268\n",
      "16641.997512212765\n",
      "16641.997512555983\n",
      "16641.9975122722\n",
      "16641.99751234559\n",
      "16641.99751247349\n",
      "16641.997512604205\n",
      "16641.997512557973\n",
      "16641.247649061326\n",
      "16641.24764884671\n",
      "16641.247648757617\n",
      "16641.247649189267\n",
      "16641.247648989196\n",
      "16641.247648919125\n",
      "16641.247649041463\n",
      "16641.247648941495\n",
      "16641.247648891338\n",
      "16641.247649152476\n",
      "16641.24764892124\n",
      "16641.247648935085\n",
      "16641.24764902949\n",
      "16641.24764909835\n",
      "16641.247648758563\n",
      "16641.247648862474\n",
      "16641.247649069377\n",
      "16641.247648824126\n",
      "16641.247649041463\n",
      "16641.247649210978\n",
      "16641.247648855875\n",
      "16641.247648951157\n",
      "16641.247649085195\n",
      "16641.247649059096\n",
      "16641.24764902949\n",
      "16641.24764897634\n",
      "16641.24764926749\n",
      "16641.247648840756\n",
      "16641.247648907523\n",
      "16641.247648919125\n",
      "16641.247649041463\n",
      "16641.24764900808\n",
      "16641.247649511562\n",
      "16641.247648541124\n",
      "16641.247648931643\n",
      "16641.247648935085\n",
      "16641.24764902949\n",
      "16641.24764858358\n",
      "16641.247649282814\n",
      "16641.247649185414\n",
      "16641.24764890851\n",
      "16641.247648966502\n",
      "16641.24764919332\n",
      "16641.247648960107\n",
      "16642.174946055016\n",
      "16642.17494613524\n",
      "16642.17494659277\n",
      "16642.174945864197\n",
      "16642.174946209034\n",
      "16642.17494620762\n",
      "16642.174946100706\n",
      "16642.174946201667\n",
      "16642.174946375268\n",
      "16642.17494589962\n",
      "16642.17494630751\n",
      "16642.174946298783\n",
      "16642.174946132156\n",
      "16642.174945694856\n",
      "16642.174946073166\n",
      "16642.17494613345\n",
      "16642.17494597138\n",
      "16642.17494625157\n",
      "16642.174946100706\n",
      "16642.174945578376\n",
      "16642.17494612977\n",
      "16642.17494603571\n",
      "16642.17494597809\n",
      "16642.174945987143\n",
      "16642.174946132156\n",
      "16642.174946321524\n",
      "16642.17494549878\n",
      "16642.174946727417\n",
      "16642.174946182375\n",
      "16642.17494620762\n",
      "16642.17494610071\n",
      "16642.174946321775\n",
      "16642.1749451016\n",
      "16642.174947010455\n",
      "16642.174946250023\n",
      "16642.174946298783\n",
      "16642.174946132152\n",
      "16642.174947311025\n",
      "16642.17494554814\n",
      "16642.17494625737\n",
      "16642.17494632859\n",
      "16642.174946264597\n",
      "16642.174946011255\n",
      "16642.174946289666\n",
      "16640.970298601915\n",
      "16640.97029848158\n",
      "16640.970298555156\n",
      "16640.97029862974\n",
      "16640.970298611435\n",
      "16640.97029854096\n",
      "16640.97029860235\n",
      "16640.970298566845\n",
      "16640.970298585362\n",
      "16640.97029861607\n",
      "16640.970298589506\n",
      "16640.970298573193\n",
      "16640.97029860451\n",
      "16640.97029849504\n",
      "16640.97029840745\n",
      "16640.970298483273\n",
      "16640.9702985861\n",
      "16640.970298515866\n",
      "16640.97029860235\n",
      "16640.970298537555\n",
      "16640.970298486533\n",
      "16640.970298514807\n",
      "16640.97029859999\n",
      "16640.970298599117\n",
      "16640.97029860451\n",
      "16640.97029861147\n",
      "16640.970298575317\n",
      "16640.970298637098\n",
      "16640.97029854486\n",
      "16640.97029854096\n",
      "16640.97029860235\n",
      "16640.97029862821\n",
      "16640.970298619435\n",
      "16640.970298506752\n",
      "16640.970298578486\n",
      "16640.970298573193\n",
      "16640.97029860451\n",
      "16640.97029864976\n",
      "16640.970298609955\n",
      "16640.97029872891\n",
      "16640.970298576798\n",
      "16640.970298591415\n",
      "16640.970298677057\n",
      "16640.970298556244\n",
      "16640.48909712978\n",
      "16640.48909705661\n",
      "16640.489097081518\n",
      "16640.489097153913\n",
      "16640.489097173937\n",
      "16640.489097116162\n",
      "16640.489097135516\n",
      "16640.489097106787\n",
      "16640.489097082984\n",
      "16640.48909715297\n",
      "16640.489097136913\n",
      "16640.48909712461\n",
      "16640.489097132897\n",
      "16640.4890970551\n",
      "16640.489097014433\n",
      "16640.489097033686\n",
      "16640.489097120968\n",
      "16640.489097117086\n",
      "16640.489097135516\n",
      "16640.489097123624\n",
      "16640.489097056092\n",
      "16640.489097057995\n",
      "16640.489097136888\n",
      "16640.48909718838\n",
      "16640.489097132897\n",
      "16640.489097155016\n",
      "16640.489097094534\n",
      "16640.489097139507\n",
      "16640.489097128437\n",
      "16640.48909711616\n",
      "16640.489097135516\n",
      "16640.489097155278\n",
      "16640.489097192363\n",
      "16640.489096976067\n",
      "16640.489097145684\n",
      "16640.48909712461\n",
      "16640.489097132893\n",
      "16640.48909711998\n",
      "16640.489097170124\n",
      "16640.489097261445\n",
      "16640.48909708602\n",
      "16640.489097098784\n",
      "16640.489097188176\n",
      "16640.489097071528\n",
      "16640.55879577571\n",
      "16640.558795503912\n",
      "16640.558795444173\n",
      "16640.55879595301\n",
      "16640.558795677014\n",
      "16640.5587953914\n",
      "16640.558795748853\n",
      "16640.558795565823\n",
      "16640.558795636833\n",
      "16640.558795878318\n",
      "16640.558795620484\n",
      "16640.558795460853\n",
      "16640.558795744113\n",
      "16640.55879587625\n",
      "16640.55879540748\n",
      "16640.558795618184\n",
      "16640.55879578047\n",
      "16640.5587953979\n",
      "16640.558795748857\n",
      "16640.55879601818\n",
      "16640.55879548479\n",
      "16640.558795718454\n",
      "16640.55879578543\n",
      "16640.55879572431\n",
      "16640.558795744113\n",
      "16640.558795551584\n",
      "16640.55879621636\n",
      "16640.558795427976\n",
      "16640.55879547563\n",
      "16640.5587953914\n",
      "16640.558795748853\n",
      "16640.558795616103\n",
      "16640.558796412064\n",
      "16640.558795241825\n",
      "16640.55879557886\n",
      "16640.558795460853\n",
      "16640.558795744113\n",
      "16640.55879499097\n",
      "16640.558796080502\n",
      "16640.558795763205\n",
      "16640.558795631954\n",
      "16640.558795652123\n",
      "16640.55879594195\n",
      "16640.558795576377\n",
      "16640.203096355533\n",
      "16640.203096208505\n",
      "16640.203096220514\n",
      "16640.203096427962\n",
      "16640.203096326622\n",
      "16640.20309621312\n",
      "16640.20309634416\n",
      "16640.203096260335\n",
      "16640.203096290777\n",
      "16640.20309640213\n",
      "16640.20309629368\n",
      "16640.203096244342\n",
      "16640.203096341247\n",
      "16640.203096350164\n",
      "16640.203096145502\n",
      "16640.203096251218\n",
      "16640.203096352776\n",
      "16640.2030962194\n",
      "16640.20309634416\n",
      "16640.203096439982\n",
      "16640.203096200108\n",
      "16640.20309630495\n",
      "16640.20309636319\n",
      "16640.203096382163\n",
      "16640.203096341247\n",
      "16640.203096294903\n",
      "16640.203096508554\n",
      "16640.203096235677\n",
      "16640.203096238372\n",
      "16640.203096213125\n",
      "16640.20309634416\n",
      "16640.203096315607\n",
      "16640.203096629546\n",
      "16640.203096078025\n",
      "16640.20309629041\n",
      "16640.203096244346\n",
      "16640.203096341244\n",
      "16640.203096048703\n",
      "16640.203096486424\n",
      "16640.203096426412\n",
      "16640.203096282814\n",
      "16640.203096293582\n",
      "16640.203096454938\n",
      "16640.20309625284\n",
      "16639.84291810791\n",
      "16639.842917999653\n",
      "16639.84291786885\n",
      "16639.842918229464\n",
      "16639.842918023398\n",
      "16639.84291791843\n",
      "16639.842918096285\n",
      "16639.842918194237\n",
      "16639.84291805607\n",
      "16639.84291816132\n",
      "16639.842917954214\n",
      "16639.842917980226\n",
      "16639.8429180983\n",
      "16639.842918114533\n",
      "16639.842917910843\n",
      "16639.84291795211\n",
      "16639.842918123268\n",
      "16639.842917880746\n",
      "16639.842918096285\n",
      "16639.842918094742\n",
      "16639.842918018316\n",
      "16639.842918009093\n",
      "16639.842918131108\n",
      "16639.84291799954\n",
      "16639.8429180983\n",
      "16639.84291798244\n",
      "16639.84291835967\n",
      "16639.84291778457\n",
      "16639.842917966023\n",
      "16639.84291791843\n",
      "16639.842918096285\n",
      "16639.842918026297\n",
      "16639.84291838113\n",
      "16639.842917672286\n",
      "16639.84291797972\n",
      "16639.842917980226\n",
      "16639.8429180983\n",
      "16639.842917869486\n",
      "16639.842918236536\n",
      "16639.84291804856\n",
      "16639.84291804591\n",
      "16639.842918089176\n",
      "16639.842918205264\n",
      "16639.842918028226\n",
      "16639.22294178131\n",
      "16639.22294169494\n",
      "16639.222941553304\n",
      "16639.222941886495\n",
      "16639.22294168894\n",
      "16639.222941618318\n",
      "16639.22294177468\n",
      "16639.22294186064\n",
      "16639.22294173502\n",
      "16639.22294182614\n",
      "16639.222941655204\n",
      "16639.222941682587\n",
      "16639.222941776476\n",
      "16639.22294174284\n",
      "16639.222941620767\n",
      "16639.22294165608\n",
      "16639.222941790504\n",
      "16639.22294156271\n",
      "16639.22294177468\n",
      "16639.222941737084\n",
      "16639.222941715292\n",
      "16639.222941707463\n",
      "16639.222941794167\n",
      "16639.22294166769\n",
      "16639.222941776476\n",
      "16639.222941662858\n",
      "16639.222941949683\n",
      "16639.222941560805\n",
      "16639.222941665103\n",
      "16639.222941618315\n",
      "16639.22294177468\n",
      "16639.22294170196\n",
      "16639.222941935644\n",
      "16639.222941522617\n",
      "16639.222941682456\n",
      "16639.222941682587\n",
      "16639.222941776476\n",
      "16639.22294167398\n",
      "16639.222941857213\n",
      "16639.222941735312\n",
      "16639.222941731867\n",
      "16639.222941776858\n",
      "16639.222941866785\n",
      "16639.222941726825\n",
      "16638.292336155522\n",
      "16638.292336177292\n",
      "16638.292336076585\n",
      "16638.292336172548\n",
      "16638.292336166887\n",
      "16638.29233617325\n",
      "16638.29233615846\n",
      "16638.292336332073\n",
      "16638.292336141465\n",
      "16638.29233615142\n",
      "16638.292336160357\n",
      "16638.292336213573\n",
      "16638.292336160866\n",
      "16638.292336106824\n",
      "16638.29233616158\n",
      "16638.292336148275\n",
      "16638.29233615557\n",
      "16638.292336139984\n",
      "16638.29233615846\n",
      "16638.292336067527\n",
      "16638.292336227358\n",
      "16638.292336145332\n",
      "16638.292336161776\n",
      "16638.29233610981\n",
      "16638.292336160866\n",
      "16638.2923361737\n",
      "16638.29233609567\n",
      "16638.29233613309\n",
      "16638.292336161903\n",
      "16638.29233617325\n",
      "16638.29233615846\n",
      "16638.29233617305\n",
      "16638.29233603417\n",
      "16638.292336151746\n",
      "16638.2923361684\n",
      "16638.292336213577\n",
      "16638.292336160866\n",
      "16638.292336499955\n",
      "16638.292336074435\n",
      "16638.292336207396\n",
      "16638.292336169463\n",
      "16638.29233622393\n",
      "16638.292336178358\n",
      "16638.292336206036\n",
      "16638.243987059832\n",
      "16638.243986984333\n",
      "16638.243986415422\n",
      "16638.243987171943\n",
      "16638.24398723781\n",
      "16638.2439870984\n",
      "16638.243987059697\n",
      "16638.243986909045\n",
      "16638.243986612226\n",
      "16638.243987159887\n",
      "16638.24398726834\n",
      "16638.243987128553\n",
      "16638.24398705958\n",
      "16638.243986986105\n",
      "16638.243986994315\n",
      "16638.243986916365\n",
      "16638.24398706105\n",
      "16638.243986913483\n",
      "16638.243987059697\n",
      "16638.243987116468\n",
      "16638.243986971796\n",
      "16638.243986972186\n",
      "16638.243987057198\n",
      "16638.243987062287\n",
      "16638.24398705958\n",
      "16638.243986976096\n",
      "16638.24398710545\n",
      "16638.24398674524\n",
      "16638.24398707578\n",
      "16638.2439870984\n",
      "16638.243987059697\n",
      "16638.24398695563\n",
      "16638.24398721397\n",
      "16638.24398682678\n",
      "16638.24398714858\n",
      "16638.243987128557\n",
      "16638.24398705958\n",
      "16638.243986941994\n",
      "16638.243987200996\n",
      "16638.243987085232\n",
      "16638.243986912985\n",
      "16638.243986959085\n",
      "16638.243987174905\n",
      "16638.243986918354\n",
      "16637.74350683368\n",
      "16637.743506846222\n",
      "16637.74350656846\n",
      "16637.74350688358\n",
      "16637.743506900595\n",
      "16637.74350687554\n",
      "16637.74350683443\n",
      "16637.74350696434\n",
      "16637.743506679704\n",
      "16637.74350686028\n",
      "16637.743506881692\n",
      "16637.743506896142\n",
      "16637.743506834253\n",
      "16637.743506795585\n",
      "16637.743506847637\n",
      "16637.743506792758\n",
      "16637.743506836894\n",
      "16637.743506799194\n",
      "16637.74350683443\n",
      "16637.7435068022\n",
      "16637.74350689465\n",
      "16637.74350680495\n",
      "16637.743506839404\n",
      "16637.743506820112\n",
      "16637.743506834257\n",
      "16637.743506822204\n",
      "16637.7435068114\n",
      "16637.743506682473\n",
      "16637.74350687447\n",
      "16637.743506875537\n",
      "16637.743506834428\n",
      "16637.743506811054\n",
      "16637.743506820483\n",
      "16637.743506692594\n",
      "16637.74350688651\n",
      "16637.743506896142\n",
      "16637.743506834257\n",
      "16637.743507053998\n",
      "16637.743506831353\n",
      "16637.743506876956\n",
      "16637.74350679617\n",
      "16637.743506848896\n",
      "16637.743506873856\n",
      "16637.74350682122\n",
      "16638.07477344879\n",
      "16638.074773587254\n",
      "16638.07477319975\n",
      "16638.074773526594\n",
      "16638.07477342939\n",
      "16638.07477327604\n",
      "16638.074773448676\n",
      "16638.074773748158\n",
      "16638.074773422377\n",
      "16638.074773447257\n",
      "16638.07477358787\n",
      "16638.074773395045\n",
      "16638.074773448367\n",
      "16638.07477323661\n",
      "16638.074773434062\n",
      "16638.074773292745\n",
      "16638.0747734587\n",
      "16638.07477303604\n",
      "16638.074773448676\n",
      "16638.074773160995\n",
      "16638.074773655942\n",
      "16638.07477346541\n",
      "16638.07477346957\n",
      "16638.074772977918\n",
      "16638.07477344837\n",
      "16638.074773257547\n",
      "16638.07477333574\n",
      "16638.074773646345\n",
      "16638.074773179458\n",
      "16638.07477327604\n",
      "16638.074773448676\n",
      "16638.07477338077\n",
      "16638.074773094606\n",
      "16638.074774217344\n",
      "16638.07477325652\n",
      "16638.07477339505\n",
      "16638.074773448367\n",
      "16638.07477436701\n",
      "16638.074773192886\n",
      "16638.074773447803\n",
      "16638.07477350083\n",
      "16638.07477365386\n",
      "16638.074773464603\n",
      "16638.074773685417\n",
      "16637.446560869776\n",
      "16637.446560932935\n",
      "16637.4465606371\n",
      "16637.446560914133\n",
      "16637.446560926786\n",
      "16637.446560879875\n",
      "16637.44656087096\n",
      "16637.446561071563\n",
      "16637.44656076886\n",
      "16637.446560876506\n",
      "16637.446560977576\n",
      "16637.446560934775\n",
      "16637.446560872373\n",
      "16637.446560779834\n",
      "16637.446560896453\n",
      "16637.446560813303\n",
      "16637.446560874996\n",
      "16637.446560733155\n",
      "16637.44656087096\n",
      "16637.4465607513\n",
      "16637.446560999477\n",
      "16637.44656087009\n",
      "16637.446560879365\n",
      "16637.446560705484\n",
      "16637.44656087237\n",
      "16637.446560831107\n",
      "16637.44656079084\n",
      "16637.446560863395\n",
      "16637.44656083897\n",
      "16637.446560879882\n",
      "16637.44656087096\n",
      "16637.446560853732\n",
      "16637.446560694883\n",
      "16637.446561091227\n",
      "16637.446560868964\n",
      "16637.446560934775\n",
      "16637.44656087237\n",
      "16637.446561399433\n",
      "16637.44656075364\n",
      "16637.44656090819\n",
      "16637.446560876742\n",
      "16637.446560968943\n",
      "16637.446560892935\n",
      "16637.446560967594\n",
      "16636.926442134907\n",
      "16636.92644221067\n",
      "16636.92644192292\n",
      "16636.926442163705\n",
      "16636.92644216388\n",
      "16636.92644213821\n",
      "16636.926442134893\n",
      "16636.926442335563\n",
      "16636.92644202675\n",
      "16636.92644212839\n",
      "16636.92644227417\n",
      "16636.926442206102\n",
      "16636.92644213488\n",
      "16636.92644203298\n",
      "16636.926442172196\n",
      "16636.92644209246\n",
      "16636.926442141605\n",
      "16636.92644200173\n",
      "16636.926442134893\n",
      "16636.92644200922\n",
      "16636.92644228723\n",
      "16636.92644216028\n",
      "16636.926442148706\n",
      "16636.926441954958\n",
      "16636.92644213488\n",
      "16636.9264421227\n",
      "16636.926442031927\n",
      "16636.92644214759\n",
      "16636.92644209053\n",
      "16636.926442138207\n",
      "16636.926442134893\n",
      "16636.926442150776\n",
      "16636.9264419261\n",
      "16636.926442452386\n",
      "16636.926442144184\n",
      "16636.926442206102\n",
      "16636.92644213488\n",
      "16636.926442762233\n",
      "16636.926441989657\n",
      "16636.926442196163\n",
      "16636.926442164397\n",
      "16636.92644225044\n",
      "16636.926442157353\n",
      "16636.926442268243\n",
      "16636.28724275989\n",
      "16636.28724277411\n",
      "16636.287242136143\n",
      "16636.287242897335\n",
      "16636.287242640563\n",
      "16636.287242607792\n",
      "16636.28724276701\n",
      "16636.287242886487\n",
      "16636.287242486065\n",
      "16636.287242839135\n",
      "16636.287242722716\n",
      "16636.28724269031\n",
      "16636.28724276529\n",
      "16636.287242555063\n",
      "16636.287242543585\n",
      "16636.287242432954\n",
      "16636.28724274763\n",
      "16636.287242442457\n",
      "16636.28724276701\n",
      "16636.28724255574\n",
      "16636.28724270443\n",
      "16636.287242545273\n",
      "16636.28724275307\n",
      "16636.287242553088\n",
      "16636.28724276529\n",
      "16636.28724245958\n",
      "16636.287242841645\n",
      "16636.287242371738\n",
      "16636.28724246937\n",
      "16636.287242607792\n",
      "16636.28724276701\n",
      "16636.28724251418\n",
      "16636.28724281366\n",
      "16636.28724262466\n",
      "16636.287242611666\n",
      "16636.28724269031\n",
      "16636.28724276529\n",
      "16636.28724287464\n",
      "16636.287242813072\n",
      "16636.287242653685\n",
      "16636.287242658324\n",
      "16636.287242751285\n",
      "16636.287242906095\n",
      "16636.28724267324\n",
      "16637.000229039724\n",
      "16637.000228793433\n",
      "16637.0002284149\n",
      "16637.00022902952\n",
      "16637.000229368703\n",
      "16637.00022905271\n",
      "16637.000229003974\n",
      "16637.00022835064\n",
      "16637.000228287106\n",
      "16637.000229131827\n",
      "16637.000229846566\n",
      "16637.000229244422\n",
      "16637.00022901278\n",
      "16637.000229228623\n",
      "16637.000228953\n",
      "16637.00022904425\n",
      "16637.000229080753\n",
      "16637.00022889469\n",
      "16637.00022900397\n",
      "16637.000229555935\n",
      "16637.00022871906\n",
      "16637.000229019708\n",
      "16637.00022907162\n",
      "16637.000229041296\n",
      "16637.00022901278\n",
      "16637.00022932657\n",
      "16637.00022883077\n",
      "16637.000229373076\n",
      "16637.00022912602\n",
      "16637.00022905271\n",
      "16637.00022900397\n",
      "16637.000229308258\n",
      "16637.000229027093\n",
      "16637.000229740835\n",
      "16637.000229345267\n",
      "16637.000229244422\n",
      "16637.00022901278\n",
      "16637.000229234447\n",
      "16637.00022906685\n",
      "16637.00022939873\n",
      "16637.00022893424\n",
      "16637.00022896539\n",
      "16637.00022917831\n",
      "16637.00022912096\n",
      "16635.921667822193\n",
      "16635.92166778373\n",
      "16635.92166716347\n",
      "16635.921667923933\n",
      "16635.92166782737\n",
      "16635.921667722047\n",
      "16635.921667826944\n",
      "16635.92166774182\n",
      "16635.921667405517\n",
      "16635.92166790827\n",
      "16635.921668013187\n",
      "16635.921667829407\n",
      "16635.921667826722\n",
      "16635.9216676883\n",
      "16635.92166764287\n",
      "16635.92166754637\n",
      "16635.92166781589\n",
      "16635.921667556973\n",
      "16635.921667826944\n",
      "16635.92166776436\n",
      "16635.92166771471\n",
      "16635.921667647286\n",
      "16635.921667816576\n",
      "16635.921667675833\n",
      "16635.92166782672\n",
      "16635.921667655984\n",
      "16635.92166782059\n",
      "16635.92166761412\n",
      "16635.921667639035\n",
      "16635.921667722043\n",
      "16635.921667826948\n",
      "16635.921667685554\n",
      "16635.921667848288\n",
      "16635.921667900064\n",
      "16635.921667803574\n",
      "16635.921667829407\n",
      "16635.92166782672\n",
      "16635.92166797187\n",
      "16635.92166786526\n",
      "16635.921667828385\n",
      "16635.92166771294\n",
      "16635.92166779106\n",
      "16635.92166797697\n",
      "16635.921667776616\n",
      "16635.55336497649\n",
      "16635.553364946492\n",
      "16635.5533644461\n",
      "16635.553364988387\n",
      "16635.553365024953\n",
      "16635.553364877607\n",
      "16635.553364961866\n",
      "16635.553364856285\n",
      "16635.553364531068\n",
      "16635.553364991098\n",
      "16635.553365344244\n",
      "16635.55336497769\n",
      "16635.55336495482\n",
      "16635.55336488311\n",
      "16635.553364911822\n",
      "16635.553364860403\n",
      "16635.553364997002\n",
      "16635.553364736665\n",
      "16635.553364961866\n",
      "16635.553364970445\n",
      "16635.553364960535\n",
      "16635.553364943746\n",
      "16635.55336500548\n",
      "16635.55336474583\n",
      "16635.553364954823\n",
      "16635.553365046082\n",
      "16635.553364813895\n",
      "16635.553364991818\n",
      "16635.55336487754\n",
      "16635.553364877607\n",
      "16635.553364961866\n",
      "16635.553365097017\n",
      "16635.55336478848\n",
      "16635.553365470343\n",
      "16635.553365045846\n",
      "16635.553364977688\n",
      "16635.55336495482\n",
      "16635.55336563349\n",
      "16635.55336488058\n",
      "16635.553365135976\n",
      "16635.55336498256\n",
      "16635.55336503225\n",
      "16635.553365077554\n",
      "16635.55336512229\n",
      "16634.891660187826\n",
      "16634.891660009154\n",
      "16634.891659533616\n",
      "16634.89166024587\n",
      "16634.89166020933\n",
      "16634.891660018206\n",
      "16634.89166017355\n",
      "16634.891659793175\n",
      "16634.891659578425\n",
      "16634.891660285175\n",
      "16634.891660554873\n",
      "16634.891660188987\n",
      "16634.891660176687\n",
      "16634.891660181784\n",
      "16634.891659983423\n",
      "16634.89166010246\n",
      "16634.89166020859\n",
      "16634.891659932706\n",
      "16634.89166017355\n",
      "16634.891660359946\n",
      "16634.891659938072\n",
      "16634.891660112058\n",
      "16634.89166020343\n",
      "16634.891660074678\n",
      "16634.891660176687\n",
      "16634.89166023865\n",
      "16634.89166007954\n",
      "16634.891660108344\n",
      "16634.891660018337\n",
      "16634.891660018202\n",
      "16634.891660173555\n",
      "16634.89166025057\n",
      "16634.89166020202\n",
      "16634.891660422014\n",
      "16634.89166027862\n",
      "16634.891660188987\n",
      "16634.891660176687\n",
      "16634.891660374476\n",
      "16634.891660209105\n",
      "16634.891660391117\n",
      "16634.891660078032\n",
      "16634.89166015204\n",
      "16634.89166035806\n",
      "16634.891660204965\n",
      "16634.749205428005\n",
      "16634.749205042906\n",
      "16634.749204252068\n",
      "16634.749205689164\n",
      "16634.749205242486\n",
      "16634.749205467437\n",
      "16634.74920546226\n",
      "16634.749204967295\n",
      "16634.74920464089\n",
      "16634.749205714073\n",
      "16634.74920523872\n",
      "16634.749205800545\n",
      "16634.749205499793\n",
      "16634.749205298493\n",
      "16634.749204761458\n",
      "16634.749205139764\n",
      "16634.749205377895\n",
      "16634.7492051277\n",
      "16634.74920546226\n",
      "16634.749205299377\n",
      "16634.74920477607\n",
      "16634.749204991527\n",
      "16634.7492053265\n",
      "16634.74920559341\n",
      "16634.749205499793\n",
      "16634.749205018135\n",
      "16634.74920566703\n",
      "16634.749204421663\n",
      "16634.749204843934\n",
      "16634.749205467437\n",
      "16634.74920546226\n",
      "16634.749204855154\n",
      "16634.749206057648\n",
      "16634.749204089127\n",
      "16634.74920520579\n",
      "16634.74920580055\n",
      "16634.749205499793\n",
      "16634.74920460204\n",
      "16634.749205848995\n",
      "16634.74920544493\n",
      "16634.74920507755\n",
      "16634.74920517738\n",
      "16634.749205775253\n",
      "16634.7492050166\n",
      "16634.487367370995\n",
      "16634.487367124173\n",
      "16634.48736658295\n",
      "16634.487367473692\n",
      "16634.487367353944\n",
      "16634.48736721082\n",
      "16634.487367364785\n",
      "16634.487366912934\n",
      "16634.487366681897\n",
      "16634.487367519876\n",
      "16634.487367634836\n",
      "16634.48736742432\n",
      "16634.48736737617\n",
      "16634.487367384965\n",
      "16634.48736705576\n",
      "16634.48736725274\n",
      "16634.48736738044\n",
      "16634.487367112764\n",
      "16634.487367364793\n",
      "16634.487367544938\n",
      "16634.4873670054\n",
      "16634.48736722172\n",
      "16634.487367363774\n",
      "16634.487367340236\n",
      "16634.48736737617\n",
      "16634.487367345693\n",
      "16634.48736733567\n",
      "16634.4873671079\n",
      "16634.487367129663\n",
      "16634.48736721082\n",
      "16634.487367364785\n",
      "16634.487367315396\n",
      "16634.48736754255\n",
      "16634.48736725955\n",
      "16634.487367427704\n",
      "16634.48736742432\n",
      "16634.48736737617\n",
      "16634.48736727583\n",
      "16634.487367488357\n",
      "16634.48736755468\n",
      "16634.487367191232\n",
      "16634.487367278496\n",
      "16634.487367589216\n",
      "16634.487367277452\n",
      "16633.713086033575\n",
      "16633.713085733427\n",
      "16633.71308520059\n",
      "16633.713086192267\n",
      "16633.71308589793\n",
      "16633.713085858493\n",
      "16633.713086033367\n",
      "16633.71308555972\n",
      "16633.713085342395\n",
      "16633.7130862294\n",
      "16633.713086116473\n",
      "16633.71308614239\n",
      "16633.713086051754\n",
      "16633.71308607796\n",
      "16633.71308559502\n",
      "16633.713085915948\n",
      "16633.713086028918\n",
      "16633.713085733507\n",
      "16633.713086033367\n",
      "16633.713086210737\n",
      "16633.713085571555\n",
      "16633.71308585321\n",
      "16633.713085999414\n",
      "16633.71308604436\n",
      "16633.713086051754\n",
      "16633.71308590495\n",
      "16633.713086071333\n",
      "16633.713085576976\n",
      "16633.713085634572\n",
      "16633.713085858493\n",
      "16633.713086033367\n",
      "16633.713085836876\n",
      "16633.713086359847\n",
      "16633.713085560725\n",
      "16633.71308599048\n",
      "16633.71308614239\n",
      "16633.713086051754\n",
      "16633.713085706142\n",
      "16633.713086234533\n",
      "16633.71308619545\n",
      "16633.713085806343\n",
      "16633.71308590726\n",
      "16633.71308630009\n",
      "16633.713085853677\n",
      "16632.9234439844\n",
      "16632.923443559925\n",
      "16632.923443036318\n",
      "16632.923444161694\n",
      "16632.92344369099\n",
      "16632.92344405685\n",
      "16632.923443988355\n",
      "16632.923443657855\n",
      "16632.923443308908\n",
      "16632.923444168136\n",
      "16632.923443696156\n",
      "16632.92344449013\n",
      "16632.923444032745\n",
      "16632.923444134005\n",
      "16632.923443412295\n",
      "16632.923443839325\n",
      "16632.9234439523\n",
      "16632.9234439082\n",
      "16632.923443988355\n",
      "16632.923444011176\n",
      "16632.923443454452\n",
      "16632.923443657153\n",
      "16632.92344389311\n",
      "16632.923444270662\n",
      "16632.923444032742\n",
      "16632.923443825395\n",
      "16632.923444223277\n",
      "16632.923443005842\n",
      "16632.9234435649\n",
      "16632.92344405685\n",
      "16632.923443988355\n",
      "16632.923443559706\n",
      "16632.923444556953\n",
      "16632.923442638636\n",
      "16632.923443823132\n",
      "16632.923444490134\n",
      "16632.923444032745\n",
      "16632.923443592506\n",
      "16632.92344432805\n",
      "16632.923444118605\n",
      "16632.923443792923\n",
      "16632.92344380996\n",
      "16632.923444277247\n",
      "16632.92344364055\n",
      "16631.628190914733\n",
      "16631.628190598178\n",
      "16631.62819029129\n",
      "16631.62819118726\n",
      "16631.628190172592\n",
      "16631.628190803738\n",
      "16631.62819093015\n",
      "16631.628190768053\n",
      "16631.628190611253\n",
      "16631.62819113039\n",
      "16631.628190191317\n",
      "16631.628191280528\n",
      "16631.62819096344\n",
      "16631.628190837215\n",
      "16631.628190214593\n",
      "16631.628190714226\n",
      "16631.62819085691\n",
      "16631.628190653108\n",
      "16631.62819093015\n",
      "16631.62819070287\n",
      "16631.628190338986\n",
      "16631.628190538977\n",
      "16631.628190823852\n",
      "16631.6281910899\n",
      "16631.62819096344\n",
      "16631.628190464053\n",
      "16631.62819124061\n",
      "16631.628189443898\n",
      "16631.628190089337\n",
      "16631.628190803738\n",
      "16631.62819093015\n",
      "16631.628190261712\n",
      "16631.62819151633\n",
      "16631.62818919487\n",
      "16631.628190449595\n",
      "16631.628191280528\n",
      "16631.628190963438\n",
      "16631.62819039085\n",
      "16631.62819129948\n",
      "16631.628190833475\n",
      "16631.628190744133\n",
      "16631.628190720792\n",
      "16631.628191262793\n",
      "16631.628190528787\n",
      "16630.714810901034\n",
      "16630.71481053203\n",
      "16630.71481092181\n",
      "16630.714811214148\n",
      "16630.714809648292\n",
      "16630.714809915007\n",
      "16630.71481084634\n",
      "16630.71481030196\n",
      "16630.714810672085\n",
      "16630.71481115812\n",
      "16630.714810199566\n",
      "16630.714810626043\n",
      "16630.714810846348\n",
      "16630.71481166796\n",
      "16630.71481067026\n",
      "16630.71481152563\n",
      "16630.714810878577\n",
      "16630.714810475984\n",
      "16630.71481084634\n",
      "16630.714811601476\n",
      "16630.714810676378\n",
      "16630.714811338115\n",
      "16630.714810861256\n",
      "16630.71481081425\n",
      "16630.714810846348\n",
      "16630.71481114787\n",
      "16630.71481077335\n",
      "16630.714810301917\n",
      "16630.71480992097\n",
      "16630.71480991501\n",
      "16630.71481084634\n",
      "16630.71481107863\n",
      "16630.714811153575\n",
      "16630.71481043962\n",
      "16630.714810553876\n",
      "16630.714810626043\n",
      "16630.714810846348\n",
      "16630.714810838595\n",
      "16630.71481086084\n",
      "16630.714811196787\n",
      "16630.714810853802\n",
      "16630.714810882233\n",
      "16630.71481115903\n",
      "16630.714810907244\n",
      "16629.330805141806\n",
      "16629.330804935897\n",
      "16629.330805455877\n",
      "16629.33080551097\n",
      "16629.330803713074\n",
      "16629.33080369119\n",
      "16629.330805093003\n",
      "16629.33080464204\n",
      "16629.330805058704\n",
      "16629.33080541253\n",
      "16629.330804455487\n",
      "16629.330804317477\n",
      "16629.33080508153\n",
      "16629.33080607012\n",
      "16629.330805074787\n",
      "16629.330806038706\n",
      "16629.33080511861\n",
      "16629.330804446425\n",
      "16629.330805092995\n",
      "16629.33080601828\n",
      "16629.330805184116\n",
      "16629.330805953883\n",
      "16629.33080511795\n",
      "16629.3308047466\n",
      "16629.33080508153\n",
      "16629.330805547386\n",
      "16629.330804712397\n",
      "16629.330804878584\n",
      "16629.330803864326\n",
      "16629.33080369119\n",
      "16629.330805093\n",
      "16629.3308056511\n",
      "16629.33080495132\n",
      "16629.330805352234\n",
      "16629.330804613495\n",
      "16629.330804317477\n",
      "16629.330805081532\n",
      "16629.330805461886\n",
      "16629.33080481899\n",
      "16629.330805461035\n",
      "16629.33080513106\n",
      "16629.330805207603\n",
      "16629.330805369675\n",
      "16629.330805357677\n",
      "16628.89136057929\n",
      "16628.891359854657\n",
      "16628.891360784117\n",
      "16628.891361073987\n",
      "16628.89135945744\n",
      "16628.891359014586\n",
      "16628.89136049737\n",
      "16628.89135871854\n",
      "16628.891360142963\n",
      "16628.891361123966\n",
      "16628.891360293266\n",
      "16628.891360193324\n",
      "16628.89136044904\n",
      "16628.891361959857\n",
      "16628.89136065002\n",
      "16628.89136184074\n",
      "16628.89136049118\n",
      "16628.891359525674\n",
      "16628.89136049737\n",
      "16628.891361716924\n",
      "16628.89136059035\n",
      "16628.891361601454\n",
      "16628.891360430323\n",
      "16628.891359796104\n",
      "16628.891360449037\n",
      "16628.891361326063\n",
      "16628.891360227455\n",
      "16628.891361011592\n",
      "16628.89135940665\n",
      "16628.891359014582\n",
      "16628.891360497368\n",
      "16628.891361049802\n",
      "16628.891361508442\n",
      "16628.89136088393\n",
      "16628.891360647787\n",
      "16628.891360193324\n",
      "16628.891360449037\n",
      "16628.891359854468\n",
      "16628.89136030595\n",
      "16628.89136111403\n",
      "16628.891360433503\n",
      "16628.891360652393\n",
      "16628.89136086397\n",
      "16628.89136088833\n",
      "16628.417769637894\n",
      "16628.417769212985\n",
      "16628.417769921554\n",
      "16628.4177700673\n",
      "16628.417768338844\n",
      "16628.41776812261\n",
      "16628.417769574375\n",
      "16628.417768549083\n",
      "16628.417769395855\n",
      "16628.417770034775\n",
      "16628.417769134085\n",
      "16628.41776898941\n",
      "16628.417769546897\n",
      "16628.417770787662\n",
      "16628.417769630854\n",
      "16628.417770710534\n",
      "16628.417769588385\n",
      "16628.41776877689\n",
      "16628.417769574375\n",
      "16628.417770692206\n",
      "16628.417769658226\n",
      "16628.41777056451\n",
      "16628.417769560376\n",
      "16628.41776907554\n",
      "16628.4177695469\n",
      "16628.417770209206\n",
      "16628.417769222706\n",
      "16628.41776971265\n",
      "16628.417768385076\n",
      "16628.41776812261\n",
      "16628.417769574375\n",
      "16628.417770172866\n",
      "16628.41776989322\n",
      "16628.417769957356\n",
      "16628.41776935038\n",
      "16628.41776898941\n",
      "16628.4177695469\n",
      "16628.417769499774\n",
      "16628.417769332314\n",
      "16628.417770054333\n",
      "16628.41776955952\n",
      "16628.41776970706\n",
      "16628.417769891024\n",
      "16628.41776988887\n",
      "16626.602110435884\n",
      "16626.602109987103\n",
      "16626.60211077462\n",
      "16626.602110870226\n",
      "16626.602109082094\n",
      "16626.602108907566\n",
      "16626.602110362015\n",
      "16626.602109342148\n",
      "16626.602110196516\n",
      "16626.60211081883\n",
      "16626.60210986891\n",
      "16626.602109779702\n",
      "16626.6021103304\n",
      "16626.6021117028\n",
      "16626.602110457905\n",
      "16626.602111562694\n",
      "16626.602110384956\n",
      "16626.602109465748\n",
      "16626.602110362008\n",
      "16626.602111534834\n",
      "16626.60211052079\n",
      "16626.602111413576\n",
      "16626.60211036248\n",
      "16626.602109806165\n",
      "16626.602110330397\n",
      "16626.602111038304\n",
      "16626.60210999319\n",
      "16626.602110586853\n",
      "16626.60210902729\n",
      "16626.602108907566\n",
      "16626.602110362008\n",
      "16626.602110986292\n",
      "16626.60211065937\n",
      "16626.602110775937\n",
      "16626.60211003925\n",
      "16626.6021097797\n",
      "16626.602110330397\n",
      "16626.602110273314\n",
      "16626.602110073673\n",
      "16626.60211082217\n",
      "16626.602110357184\n",
      "16626.60211051849\n",
      "16626.602110688236\n",
      "16626.602110701802\n",
      "16623.66418474455\n",
      "16623.664184213238\n",
      "16623.66418535651\n",
      "16623.66418525039\n",
      "16623.664183139957\n",
      "16623.664183628392\n",
      "16623.664184589055\n",
      "16623.664183927973\n",
      "16623.66418467858\n",
      "16623.66418508546\n",
      "16623.664183794004\n",
      "16623.664184375008\n",
      "16623.664184574252\n",
      "16623.664186721202\n",
      "16623.664184822694\n",
      "16623.66418619281\n",
      "16623.664184678422\n",
      "16623.664183160534\n",
      "16623.664184589055\n",
      "16623.6641863003\n",
      "16623.664184972044\n",
      "16623.664186058402\n",
      "16623.6641847212\n",
      "16623.66418381897\n",
      "16623.664184574252\n",
      "16623.664185439113\n",
      "16623.664184119447\n",
      "16623.664185283862\n",
      "16623.66418217383\n",
      "16623.664183628392\n",
      "16623.664184589055\n",
      "16623.66418550064\n",
      "16623.664184367022\n",
      "16623.66418547498\n",
      "16623.664183265875\n",
      "16623.664184375008\n",
      "16623.664184574252\n",
      "16623.664184881025\n",
      "16623.66418409875\n",
      "16623.66418492428\n",
      "16623.664184735437\n",
      "16623.66418484504\n",
      "16623.66418495162\n",
      "16623.664184866193\n",
      "16636.183641796164\n",
      "16636.183641405645\n",
      "16636.1836421831\n",
      "16636.18364171213\n",
      "16636.183638723145\n",
      "16636.18363776941\n",
      "16636.183641736196\n",
      "16636.18363957315\n",
      "16636.183641195472\n",
      "16636.183641089\n",
      "16636.18363888804\n",
      "16636.18363641559\n",
      "16636.18364180266\n",
      "16636.18364386075\n",
      "16636.183641875028\n",
      "16636.183643124223\n",
      "16636.183641998487\n",
      "16636.18363939942\n",
      "16636.1836417362\n",
      "16636.18364168368\n",
      "16636.183642174365\n",
      "16636.183642358097\n",
      "16636.183642246488\n",
      "16636.183639189265\n",
      "16636.18364180266\n",
      "16636.183643168275\n",
      "16636.18364047416\n",
      "16636.18364233204\n",
      "16636.18363884313\n",
      "16636.18363776941\n",
      "16636.1836417362\n",
      "16636.183642363652\n",
      "16636.183642167485\n",
      "16636.183640482202\n",
      "16636.183640644336\n",
      "16636.18363641559\n",
      "16636.18364180266\n",
      "16636.183640483836\n",
      "16636.183640505416\n",
      "16636.183642578377\n",
      "16636.183641596846\n",
      "16636.183642053427\n",
      "16636.183641867538\n",
      "16636.183642480413\n",
      "16621.99545044941\n",
      "16621.995449973547\n",
      "16621.995451026487\n",
      "16621.995450842973\n",
      "16621.99544851463\n",
      "16621.995448892914\n",
      "16621.995450313032\n",
      "16621.995449512335\n",
      "16621.995450307164\n",
      "16621.995450623144\n",
      "16621.995449113165\n",
      "16621.9954492469\n",
      "16621.995450305607\n",
      "16621.995452414136\n",
      "16621.995450512368\n",
      "16621.995451862847\n",
      "16621.9954504302\n",
      "16621.995448759553\n",
      "16621.995450313032\n",
      "16621.995451816827\n",
      "16621.995450692088\n",
      "16621.995451662435\n",
      "16621.99545049925\n",
      "16621.995449292568\n",
      "16621.995450305607\n",
      "16621.99545126389\n",
      "16621.995449705762\n",
      "16621.995450915194\n",
      "16621.995447809943\n",
      "16621.99544889291\n",
      "16621.995450313032\n",
      "16621.99545130459\n",
      "16621.99545007227\n",
      "16621.995450897364\n",
      "16621.995448999944\n",
      "16621.9954492469\n",
      "16621.995450305607\n",
      "16621.995450412192\n",
      "16621.99544969597\n",
      "16621.99545068959\n",
      "16621.995450416172\n",
      "16621.99545058225\n",
      "16621.995450619695\n",
      "16621.995450685976\n",
      "16619.28768384008\n",
      "16619.287683473107\n",
      "16619.287684421077\n",
      "16619.28768419782\n",
      "16619.287681927657\n",
      "16619.287682873943\n",
      "16619.28768369473\n",
      "16619.287683443545\n",
      "16619.28768389636\n",
      "16619.287684006016\n",
      "16619.287682386468\n",
      "16619.287683044582\n",
      "16619.28768371403\n",
      "16619.287685541043\n",
      "16619.28768369821\n",
      "16619.28768506012\n",
      "16619.28768378912\n",
      "16619.28768227897\n",
      "16619.28768369473\n",
      "16619.287685173404\n",
      "16619.287683827133\n",
      "16619.287684934032\n",
      "16619.28768386692\n",
      "16619.28768291405\n",
      "16619.287683714032\n",
      "16619.287684472933\n",
      "16619.287683196097\n",
      "16619.287684131465\n",
      "16619.28768110979\n",
      "16619.28768287395\n",
      "16619.28768369473\n",
      "16619.287684689672\n",
      "16619.287683092578\n",
      "16619.287684401304\n",
      "16619.287682033606\n",
      "16619.287683044582\n",
      "16619.287683714032\n",
      "16619.28768403582\n",
      "16619.287683195835\n",
      "16619.287683871145\n",
      "16619.287683872266\n",
      "16619.287683919763\n",
      "16619.28768396855\n",
      "16619.287683806917\n",
      "16615.493356507006\n",
      "16615.49335590762\n",
      "16615.49335679867\n",
      "16615.493356608447\n",
      "16615.493355367034\n",
      "16615.493356017807\n",
      "16615.493356317966\n",
      "16615.49335551281\n",
      "16615.493356315645\n",
      "16615.49335650907\n",
      "16615.493355775598\n",
      "16615.493356654682\n",
      "16615.493356319468\n",
      "16615.493358000687\n",
      "16615.493356652165\n",
      "16615.493357697393\n",
      "16615.49335647218\n",
      "16615.49335492499\n",
      "16615.493356317966\n",
      "16615.49335720277\n",
      "16615.493356825664\n",
      "16615.49335754718\n",
      "16615.493356526855\n",
      "16615.49335542705\n",
      "16615.49335631947\n",
      "16615.49335715841\n",
      "16615.493356013376\n",
      "16615.493357491065\n",
      "16615.493354402723\n",
      "16615.493356017807\n",
      "16615.493356317966\n",
      "16615.49335687653\n",
      "16615.49335654393\n",
      "16615.493356989\n",
      "16615.493355330873\n",
      "16615.493356654682\n",
      "16615.493356319468\n",
      "16615.493355976552\n",
      "16615.49335588311\n",
      "16615.49335662624\n",
      "16615.493356428753\n",
      "16615.493356610383\n",
      "16615.49335662287\n",
      "16615.493356699248\n",
      "16618.591172974226\n",
      "16618.591174140383\n",
      "16618.591175642785\n",
      "16618.591173563746\n",
      "16618.591173717126\n",
      "16618.59117170464\n",
      "16618.591172937573\n",
      "16618.59117350353\n",
      "16618.591174716257\n",
      "16618.591173195033\n",
      "16618.591173223875\n",
      "16618.591171948825\n",
      "16618.591172956283\n",
      "16618.591174886526\n",
      "16618.591172218905\n",
      "16618.59117302057\n",
      "16618.59117245175\n",
      "16618.591169863375\n",
      "16618.591172937573\n",
      "16618.591174852158\n",
      "16618.59117264857\n",
      "16618.591173236175\n",
      "16618.591172651526\n",
      "16618.5911707311\n",
      "16618.591172956287\n",
      "16618.59117155838\n",
      "16618.591172765893\n",
      "16618.59117410134\n",
      "16618.59116870152\n",
      "16618.591171704644\n",
      "16618.591172937573\n",
      "16618.591171781227\n",
      "16618.591173706664\n",
      "16618.591173219676\n",
      "16618.59117037256\n",
      "16618.591171948832\n",
      "16618.591172956287\n",
      "16618.591170497286\n",
      "16618.59117316993\n",
      "16618.59117179998\n",
      "16618.591172781897\n",
      "16618.59117289792\n",
      "16618.591173377572\n",
      "16618.591173843546\n",
      "16613.758627440544\n",
      "16613.75862746936\n",
      "16613.758628606956\n",
      "16613.758627683088\n",
      "16613.75862679052\n",
      "16613.75862676115\n",
      "16613.758627311643\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[139], line 9\u001b[0m\n\u001b[0;32m      4\u001b[0m regularization_parameter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      5\u001b[0m problem_parameters \u001b[38;5;241m=\u001b[39m (g_paper, f_initial, regularization_parameter)\n\u001b[1;32m----> 9\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimization_parameters_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_span\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43mqnn_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproblem_parameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdisp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaxiter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\scipy\\optimize\\_minimize.py:691\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    689\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_cg(fun, x0, args, jac, callback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbfgs\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 691\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_bfgs(fun, x0, args, jac, callback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    692\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewton-cg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    693\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    694\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\scipy\\optimize\\_optimize.py:1388\u001b[0m, in \u001b[0;36m_minimize_bfgs\u001b[1;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, finite_diff_rel_step, xrtol, **unknown_options)\u001b[0m\n\u001b[0;32m   1385\u001b[0m pk \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mdot(Hk, gfk)\n\u001b[0;32m   1386\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1387\u001b[0m     alpha_k, fc, gc, old_fval, old_old_fval, gfkp1 \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m-> 1388\u001b[0m              \u001b[43m_line_search_wolfe12\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmyfprime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgfk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1389\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mold_fval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mold_old_fval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mamin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mamax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _LineSearchError:\n\u001b[0;32m   1391\u001b[0m     \u001b[38;5;66;03m# Line search failed to find a better solution.\u001b[39;00m\n\u001b[0;32m   1392\u001b[0m     warnflag \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\scipy\\optimize\\_optimize.py:1160\u001b[0m, in \u001b[0;36m_line_search_wolfe12\u001b[1;34m(f, fprime, xk, pk, gfk, old_fval, old_old_fval, **kwargs)\u001b[0m\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;124;03mSame as line_search_wolfe1, but fall back to line_search_wolfe2 if\u001b[39;00m\n\u001b[0;32m   1148\u001b[0m \u001b[38;5;124;03msuitable step length is not found, and raise an exception if a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1155\u001b[0m \n\u001b[0;32m   1156\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m extra_condition \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextra_condition\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1160\u001b[0m ret \u001b[38;5;241m=\u001b[39m line_search_wolfe1(f, fprime, xk, pk, gfk,\n\u001b[0;32m   1161\u001b[0m                          old_fval, old_old_fval,\n\u001b[0;32m   1162\u001b[0m                          \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m extra_condition \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1165\u001b[0m     xp1 \u001b[38;5;241m=\u001b[39m xk \u001b[38;5;241m+\u001b[39m ret[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m pk\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\scipy\\optimize\\_linesearch.py:84\u001b[0m, in \u001b[0;36mline_search_wolfe1\u001b[1;34m(f, fprime, xk, pk, gfk, old_fval, old_old_fval, args, c1, c2, amax, amin, xtol)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mdot(gval[\u001b[38;5;241m0\u001b[39m], pk)\n\u001b[0;32m     82\u001b[0m derphi0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(gfk, pk)\n\u001b[1;32m---> 84\u001b[0m stp, fval, old_fval \u001b[38;5;241m=\u001b[39m \u001b[43mscalar_search_wolfe1\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mphi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mderphi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mold_fval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mold_old_fval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mderphi0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mc1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mamax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mamin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxtol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stp, fc[\u001b[38;5;241m0\u001b[39m], gc[\u001b[38;5;241m0\u001b[39m], fval, old_fval, gval[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\scipy\\optimize\\_linesearch.py:161\u001b[0m, in \u001b[0;36mscalar_search_wolfe1\u001b[1;34m(phi, derphi, phi0, old_phi0, derphi0, c1, c2, amax, amin, xtol)\u001b[0m\n\u001b[0;32m    159\u001b[0m     alpha1 \u001b[38;5;241m=\u001b[39m stp\n\u001b[0;32m    160\u001b[0m     phi1 \u001b[38;5;241m=\u001b[39m phi(stp)\n\u001b[1;32m--> 161\u001b[0m     derphi1 \u001b[38;5;241m=\u001b[39m \u001b[43mderphi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\scipy\\optimize\\_linesearch.py:78\u001b[0m, in \u001b[0;36mline_search_wolfe1.<locals>.derphi\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mderphi\u001b[39m(s):\n\u001b[1;32m---> 78\u001b[0m     gval[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfprime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     gc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mdot(gval[\u001b[38;5;241m0\u001b[39m], pk)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\scipy\\optimize\\_differentiable_functions.py:273\u001b[0m, in \u001b[0;36mScalarFunction.grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx):\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x_impl(x)\n\u001b[1;32m--> 273\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\scipy\\optimize\\_differentiable_functions.py:256\u001b[0m, in \u001b[0;36mScalarFunction._update_grad\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_grad\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_updated:\n\u001b[1;32m--> 256\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_grad_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\scipy\\optimize\\_differentiable_functions.py:173\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_grad\u001b[1;34m()\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun()\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 173\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg \u001b[38;5;241m=\u001b[39m approx_derivative(fun_wrapped, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx, f0\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf,\n\u001b[0;32m    174\u001b[0m                            \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfinite_diff_options)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\scipy\\optimize\\_numdiff.py:505\u001b[0m, in \u001b[0;36mapprox_derivative\u001b[1;34m(fun, x0, method, rel_step, abs_step, f0, bounds, sparsity, as_linear_operator, args, kwargs)\u001b[0m\n\u001b[0;32m    502\u001b[0m     use_one_sided \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparsity \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_dense_difference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_wrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m                             \u001b[49m\u001b[43muse_one_sided\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    508\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issparse(sparsity) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sparsity) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\scipy\\optimize\\_numdiff.py:576\u001b[0m, in \u001b[0;36m_dense_difference\u001b[1;34m(fun, x0, f0, h, use_one_sided, method)\u001b[0m\n\u001b[0;32m    574\u001b[0m     x \u001b[38;5;241m=\u001b[39m x0 \u001b[38;5;241m+\u001b[39m h_vecs[i]\n\u001b[0;32m    575\u001b[0m     dx \u001b[38;5;241m=\u001b[39m x[i] \u001b[38;5;241m-\u001b[39m x0[i]  \u001b[38;5;66;03m# Recompute dx as exactly representable number.\u001b[39;00m\n\u001b[1;32m--> 576\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m f0\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3-point\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m use_one_sided[i]:\n\u001b[0;32m    578\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m x0 \u001b[38;5;241m+\u001b[39m h_vecs[i]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\scipy\\optimize\\_numdiff.py:456\u001b[0m, in \u001b[0;36mapprox_derivative.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfun_wrapped\u001b[39m(x):\n\u001b[1;32m--> 456\u001b[0m     f \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39matleast_1d(fun(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m    457\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`fun` return value has \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m                            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmore than 1 dimension.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\scipy\\optimize\\_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "Cell \u001b[1;32mIn[135], line 5\u001b[0m, in \u001b[0;36mtraining_loss\u001b[1;34m(optimization_parameters, X_train, qnn_, problem_parameters)\u001b[0m\n\u001b[0;32m      3\u001b[0m g, f_initial, regularization_parameter \u001b[38;5;241m=\u001b[39m problem_parameters\n\u001b[0;32m      4\u001b[0m output_f \u001b[38;5;241m=\u001b[39m qnn_\u001b[38;5;241m.\u001b[39mevaluate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m, X_train, param_ini, param_op_ini)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m----> 5\u001b[0m output_dfdx \u001b[38;5;241m=\u001b[39m \u001b[43mqnn_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdfdx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_ini\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_op_ini\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdfdx\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss_function(output_f, output_dfdx, X_train, g, f_initial, regularization_parameter)\n",
      "File \u001b[1;32m~\\Desktop\\Qiskit\\squlearnRF\\src\\squlearn\\qnn\\qnn.py:935\u001b[0m, in \u001b[0;36mQNN.evaluate\u001b[1;34m(self, values, x, param, param_op)\u001b[0m\n\u001b[0;32m    931\u001b[0m     val \u001b[38;5;241m=\u001b[39m OpTree\u001b[38;5;241m.\u001b[39mevaluate\u001b[38;5;241m.\u001b[39mevaluate_with_sampler(\n\u001b[0;32m    932\u001b[0m         pqc_optree, operators, dict_encoding_circuit, dict_operator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler\n\u001b[0;32m    933\u001b[0m     )\n\u001b[0;32m    934\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_estimator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 935\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[43mOpTree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_with_estimator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    936\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpqc_optree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdict_encoding_circuit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdict_operator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_estimator\u001b[49m\n\u001b[0;32m    937\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    939\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo execution is set!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Desktop\\Qiskit\\squlearnRF\\src\\squlearn\\util\\optree\\optree_evaluate.py:1222\u001b[0m, in \u001b[0;36mOpTreeEvaluate.evaluate_with_estimator\u001b[1;34m(circuit, operator, dictionary_circuit, dictionary_operator, estimator, dictionaries_combined, detect_duplicates)\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(total_circuit_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([])\n\u001b[0;32m   1221\u001b[0m estimator_result \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1222\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_circuit_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_operator_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_parameter_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1223\u001b[0m     \u001b[38;5;241m.\u001b[39mresult()\n\u001b[0;32m   1224\u001b[0m     \u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m   1225\u001b[0m )\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;66;03m# print(\"Estimator run time: \", time.time() - start)\u001b[39;00m\n\u001b[0;32m   1227\u001b[0m \n\u001b[0;32m   1228\u001b[0m \u001b[38;5;66;03m# Assembly the final values from the evaluation tree\u001b[39;00m\n\u001b[0;32m   1229\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[1;32m~\\Desktop\\Qiskit\\squlearnRF\\src\\squlearn\\util\\executor.py:1131\u001b[0m, in \u001b[0;36mExecutorEstimator.run\u001b[1;34m(self, circuits, observables, parameter_values, **run_options)\u001b[0m\n\u001b[0;32m   1117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\n\u001b[0;32m   1118\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1119\u001b[0m     circuits,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrun_options,\n\u001b[0;32m   1123\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Job:\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;124;03m    Overwrites the sampler primitive run method, to evaluate expectation values.\u001b[39;00m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;124;03m    Uses the Executor class for automatic session handling.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1129\u001b[0m \n\u001b[0;32m   1130\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor\u001b[38;5;241m.\u001b[39mestimator_run(\n\u001b[0;32m   1132\u001b[0m         circuits\u001b[38;5;241m=\u001b[39mcircuits,\n\u001b[0;32m   1133\u001b[0m         observables\u001b[38;5;241m=\u001b[39mobservables,\n\u001b[0;32m   1134\u001b[0m         parameter_values\u001b[38;5;241m=\u001b[39mparameter_values,\n\u001b[0;32m   1135\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrun_options,\n\u001b[0;32m   1136\u001b[0m     )\n",
      "File \u001b[1;32m~\\Desktop\\Qiskit\\squlearnRF\\src\\squlearn\\util\\executor.py:734\u001b[0m, in \u001b[0;36mExecutor.estimator_run\u001b[1;34m(self, circuits, observables, parameter_values, **kwargs)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    732\u001b[0m     hash_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 734\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_primitive_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mestimator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhash_value\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\Qiskit\\squlearnRF\\src\\squlearn\\util\\executor.py:608\u001b[0m, in \u001b[0;36mExecutor._primitive_run\u001b[1;34m(self, run, label, hash_value)\u001b[0m\n\u001b[0;32m    606\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    607\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 608\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Job is completed, check if it was successful\u001b[39;00m\n\u001b[0;32m    611\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m==\u001b[39m JobStatus\u001b[38;5;241m.\u001b[39mERROR:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "param_ini, param_op_ini = np.random.rand(2*num_qubits*num_layers), np.random.rand(num_qubits+1)\n",
    "optimization_parameters_0 = np.concatenate((param_ini, param_op_ini))\n",
    "f_initial = 1\n",
    "regularization_parameter = 1\n",
    "problem_parameters = (g_paper, f_initial, regularization_parameter)\n",
    "\n",
    "\n",
    "\n",
    "result = minimize(training_loss, optimization_parameters_0, args=(x_span,\n",
    "            qnn_, problem_parameters),\n",
    "            options={'disp': True, 'maxiter': 10000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = qnn_.evaluate(\"f\", x_span, result.x[:qnn_.num_parameters], result.x[qnn_.num_parameters:])[\"f\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x181fb505180>]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_span, solution, label=\"QNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QNN ODE solver \n",
    "\n",
    "# Calculate ground truth based on derivatives in the PINN sense (see already implemented with kernels)\n",
    "# use partial_fit from qnn.qnnr.QNNRegressor, y_values will be calculated as above\n",
    "\n",
    "# based on partial_fit, create new loss function that calculates the derivatives of the QNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_FQK_kernel_derivatives(self, x_array, qnn_, coef):\n",
    "    \"\"\"\n",
    "    Get the FQK kernel and its derivatives for the given input data.\n",
    "\n",
    "    Parameters:\n",
    "    - x_array: The input data. np.array of shape (n, m) where n is the number of samples and m is the number of features.\n",
    "    - qnn_: The FQK QNN.\n",
    "    - coef: The coefficients of the P0 observable to be used in the QNN squlearn evaluation\n",
    "\n",
    "    Returns:\n",
    "    - output_f: The FQK kernel.\n",
    "    - output_dfdx: The derivatives of the FQK kernel:  shape (n, n, m*2),  the last dimension is the derivative with respect to the input data. \n",
    "    \"\"\"\n",
    "    x_array = x_array.reshape(-1, 1) #reshape to column vector\n",
    "    x_list_circuit_format = self.x_to_circuit_format(x_array)\n",
    "\n",
    "\n",
    "    output_f = qnn_.evaluate(\"f\", x_list_circuit_format, [], coef)[\"f\"]\n",
    "    output_dfdx = qnn_.evaluate(\"dfdx\", x_list_circuit_format, [], coef)[\"dfdx\"]\n",
    "    \n",
    "\n",
    "    #reshape the output to the shape of the gram matrix\n",
    "    output_f = output_f.reshape((len(x_array), len(x_array)))\n",
    "    output_dfdx = output_dfdx.reshape((len(x_array), len(x_array), len(x_array[0])*2))\n",
    "\n",
    "\n",
    "    return output_f, output_dfdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New squlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from squlearn.qnn.lowlevel_qnn_base import LowLevelQNNBase\n",
    "from squlearn.qnn.lowlevel_qnn_qiskit import LowLevelQNNQiskit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        #qnn_ = BaseQNN(FQK_Circuit, P0_, self.executor, result_caching=False, optree_caching=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_circuit = ChebyshevRx(num_qubits, num_features, num_layers)\n",
    "operator = SummedPaulis(num_qubits),\n",
    "executor = Executor(\"statevector_simulator\")\n",
    "loss = SquaredLoss(),\n",
    "optimizer = SLSQP(),\n",
    "param_ini = np.random.rand(2*num_qubits*num_layers),\n",
    "param_op_ini = np.random.rand(num_qubits+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mInit signature:\u001b[0m\n",
      "\u001b[0mLowLevelQNNQiskit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mparameterized_quantum_circuit\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msqulearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding_circuit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding_circuit_base\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEncodingCircuitBase\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0moperator\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msqulearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservable_base\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mObservableBase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mexecutor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msqulearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExecutor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0moptree_caching\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mresult_caching\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m     \n",
      "Low level implementation of QNNs and its derivatives based on Qiskit.\n",
      "\n",
      "Args:\n",
      "    pqc (EncodingCircuitBase) : parameterized quantum circuit in encoding circuit format\n",
      "    operator (Union[ObservableBase,list]): Operator that is used in the expectation\n",
      "        value of the QNN. Can be a list for multiple outputs.\n",
      "    executor (Executor) : Executor that is used for the evaluation of the QNN\n",
      "    optree_caching : Caching of the optree expressions (default = True recommended)\n",
      "    result_caching : Caching of the result for each `x`, `param`, `param_op` combination\n",
      "        (default = True)\n",
      "\n",
      "Attributes:\n",
      "-----------\n",
      "\n",
      "Attributes:\n",
      "    num_qubits (int): Number of qubits of the QNN\n",
      "    num_features (int): Dimension of features of the PQC\n",
      "    num_parameters (int): Number of trainable parameters of the PQC\n",
      "    num_operator (int): Number of outputs\n",
      "    num_parameters_observable (int): Number of trainable parameters of the expectation value operator\n",
      "    multiple_output (bool): True if multiple outputs are used\n",
      "    parameters (ParameterVector): Parameter vector of the PQC\n",
      "    features (ParameterVector): Feature vector of the PQC\n",
      "    parameters_operator (ParameterVector): Parameter vector of the cost operator\n",
      "\n",
      "Methods:\n",
      "--------\n",
      "\u001b[1;31mFile:\u001b[0m           c:\\users\\jsl-rf\\desktop\\kernelsde\\.venv\\lib\\site-packages\\squlearn\\qnn\\lowlevel_qnn_qiskit.py\n",
      "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
      "\u001b[1;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "LowLevelQNNQiskit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'set_map'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m qnn \u001b[38;5;241m=\u001b[39m \u001b[43mLowLevelQNNQiskit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameterized_quantum_circuit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mencoding_circuit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                        \u001b[49m\u001b[43moperator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mexecutor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mexecutor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mresult_caching\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                        \u001b[49m\u001b[43moptree_caching\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jsl-rf\\Desktop\\KernelsDE\\.venv\\lib\\site-packages\\squlearn\\qnn\\lowlevel_qnn_qiskit.py:323\u001b[0m, in \u001b[0;36mLowLevelQNNQiskit.__init__\u001b[1;34m(self, parameterized_quantum_circuit, operator, executor, optree_caching, result_caching)\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor\u001b[38;5;241m.\u001b[39mget_sampler()\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 323\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initilize_derivative\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jsl-rf\\Desktop\\KernelsDE\\.venv\\lib\\site-packages\\squlearn\\qnn\\lowlevel_qnn_qiskit.py:405\u001b[0m, in \u001b[0;36mLowLevelQNNQiskit._initilize_derivative\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    403\u001b[0m         num_qubits_operator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(num_qubits_operator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_observable[i]\u001b[38;5;241m.\u001b[39mnum_qubits)\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 405\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_observable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_map\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pqc\u001b[38;5;241m.\u001b[39mqubit_map, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pqc\u001b[38;5;241m.\u001b[39mnum_physical_qubits)\n\u001b[0;32m    406\u001b[0m     num_qubits_operator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_observable\u001b[38;5;241m.\u001b[39mnum_qubits\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperator_derivatives \u001b[38;5;241m=\u001b[39m ObservableDerivatives(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_observable, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optree_caching)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'set_map'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mInit signature:\u001b[0m\n",
      "\u001b[0mBaseQNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mencoding_circuit\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'EncodingCircuitBase'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0moperator\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Union[ObservableBase, list[ObservableBase]]'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mexecutor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Executor'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mloss\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'LossBase'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0moptimizer\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'OptimizerBase'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mparam_ini\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Union[np.ndarray, None]'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mparam_op_ini\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Union[np.ndarray, None]'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'int'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mepochs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'int'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mshuffle\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mopt_param_op\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mvariance\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Union[float, Callable]'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mshot_control\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'ShotControlBase'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mparameter_seed\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Union[int, None]'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcaching\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mpretrained\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Union[Callable, str, None]'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m'None'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m     \n",
      "Base Class for Quantum Neural Networks.\n",
      "\n",
      "Args:\n",
      "    encoding_circuit : Parameterized quantum circuit in encoding circuit format\n",
      "    operator : Operator that are used in the expectation value of the QNN. Can be a list for\n",
      "        multiple outputs.\n",
      "    executor : Executor instance\n",
      "    optimizer : Optimizer instance\n",
      "    param_ini : Initialization values of the parameters of the PQC\n",
      "    param_op_ini : Initialization values of the cost operator\n",
      "    batch_size : Number of data points in each batch, for SGDMixin optimizers\n",
      "    epochs : Number of epochs of SGD to perform, for SGDMixin optimizers\n",
      "    shuffle : If True, data points get shuffled before each epoch (default: False),\n",
      "        for SGDMixin optimizers\n",
      "    opt_param_op : If True, operators parameters get optimized\n",
      "    variance : Variance factor\n",
      "    parameter_seed : Seed for the random number generator for the parameter initialization\n",
      "    caching : If True, the results of the QNN are cached.\n",
      "    pretrained : Set to true if the supplied parameters are already trained.\n",
      "    callback (Union[Callable, str, None], default=None): A callback for the optimization loop.\n",
      "        Can be either a Callable, \"pbar\" (which uses a :class:`tqdm.tqdm` process bar) or None.\n",
      "        If None, the optimizers (default) callback will be used.\n",
      "\u001b[1;31mFile:\u001b[0m           c:\\users\\jsl-rf\\desktop\\kernelsde\\.venv\\lib\\site-packages\\squlearn\\qnn\\base_qnn.py\n",
      "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
      "\u001b[1;31mSubclasses:\u001b[0m     QNNClassifier, QNNRegressor"
     ]
    }
   ],
   "source": [
    "BaseQNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_FQK_kernel_derivatives(self, x_array, qnn_, coef):\n",
    "        \"\"\"\n",
    "        Get the FQK kernel and its derivatives for the given input data.\n",
    "\n",
    "        Parameters:\n",
    "        - x_array: The input data. np.array of shape (n, m) where n is the number of samples and m is the number of features.\n",
    "        - qnn_: The FQK QNN.\n",
    "        - coef: The coefficients of the P0 observable to be used in the QNN squlearn evaluation\n",
    "\n",
    "        Returns:\n",
    "        - output_f: The FQK kernel.\n",
    "        - output_dfdx: The derivatives of the FQK kernel:  shape (n, n, m*2),  the last dimension is the derivative with respect to the input data. \n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        x_array = x_array.reshape(-1, 1) #reshape to column vector\n",
    "        x_list_circuit_format = self.x_to_circuit_format(x_array)\n",
    "\n",
    "\n",
    "        output_f = qnn_.evaluate(\"f\", x_list_circuit_format, [], coef)[\"f\"]\n",
    "        output_dfdx = qnn_.evaluate(\"dfdx\", x_list_circuit_format, [], coef)[\"dfdx\"]\n",
    "        \n",
    "\n",
    "        #reshape the output to the shape of the gram matrix\n",
    "        output_f = output_f.reshape((len(x_array), len(x_array)))\n",
    "        output_dfdx = output_dfdx.reshape((len(x_array), len(x_array), len(x_array[0])*2))\n",
    "\n",
    "\n",
    "        return output_f, output_dfdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QNN ODE solver \n",
    "\n",
    "# Calculate ground truth based on derivatives in the PINN sense (see already implemented with kernels)\n",
    "# use partial_fit from qnn.qnnr.QNNRegressor, y_values will be calculated as above\n",
    "\n",
    "# based on partial_fit, create new loss function that calculates the derivatives of the QNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
